{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13504"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import plotly.offline as pyo\n",
    "import plotly.express as px\n",
    "import dimensionality_reduction\n",
    "import json \n",
    "import random\n",
    "import fast_hdbscan\n",
    "\n",
    "# Load the pickled dictionary\n",
    "with open('../glanos-data/big_consulting_export-sbert.pickle', 'rb') as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "json_file_path = \"../glanos-data/big_consulting_export.json\"\n",
    "\n",
    "with open(json_file_path, 'r') as j:\n",
    "     contents = json.loads(j.read())\n",
    "\n",
    "# Get the list of item objects\n",
    "items = contents['items']\n",
    "df = pd.DataFrame(items)\n",
    "df[\"embedding\"] = df[\"snippet\"].map(embeddings)\n",
    "df.dropna(inplace=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 50\n",
    "df = df.sample(n=sample_size, random_state=42)\n",
    "# df.to_csv(f'big_consulting_export_clustered-sample-50.tsv', sep=\"\\t\", index=True)\n",
    "\n",
    "sample_dict = dict(random.sample(embeddings.items(), sample_size))\n",
    "sample_sent = \"conducted a survey about sustainable supply chains with 525 large corporations across Argentina, Brazil, Canada, Mexico and the United States\"\n",
    "company_names = [    'Louis Dreyfus Company',    'Lenovo Group Limited',    'Marubeni Corp.',    'LUKOIL PJSC',    'Midea Group Company Limited',    'Medtronic PLC Holding',    'Medipal Holding Corp.',    'Mitsubishi Corp.',    'NestlÃ¨ SA',    'Oil & Natural gas Corp.',    'Pegatron Corp.',    'POSCO Group',    'PACCAR Inc.',    'Pemex',    \"People's Insurance Company of China\",    'Quanta Computer Inc.',    'Plains GP Holding LP',\n",
    "                'Apple', 'Microsoft', 'Aldi', 'Tesco', 'Mercadona', 'BeReal']\n",
    "sample_sents = []\n",
    "for company in company_names:\n",
    "    sample_sents.append(company+\" \"+sample_sent)\n",
    "\n",
    "with open('companies.pkl', 'wb') as f:\n",
    "    pickle.dump(sample_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import pandas as pd\n",
    "from hdbscan import flat\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def calculate_cluster(df, cols=[\"x\", \"y\", \"z\"], n_clusters=None, *args, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cluster data using hdbscan\n",
    "    :param df:\n",
    "    :param cols:\n",
    "    :param args:\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"cluster: \", cols, args, kwargs)\n",
    "    \n",
    "    if n_clusters is None:\n",
    "        clusterer = hdbscan.HDBSCAN(*args, **kwargs)\n",
    "        clusterer.fit(df[cols].to_numpy())\n",
    "    else:\n",
    "        clusterer = flat.HDBSCAN_flat(df[cols], cluster_selection_method='leaf', n_clusters=n_clusters)\n",
    "    df[\"cluster_id\"] = clusterer.labels_\n",
    "    df[\"cluster_id\"] = df[\"cluster_id\"].astype(str)\n",
    "    df[\"cluster_prob\"] = clusterer.probabilities_\n",
    "\n",
    "\n",
    "    # print amount of clusters\n",
    "    print(\"amount clusters: \", len(df[\"cluster_id\"].unique()))\n",
    "    return df, clusterer\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import clustering\n",
    "n_clusters = 33\n",
    "df_split = pd.DataFrame(df['embedding'].to_list())\n",
    "\n",
    "# Concatenate the split columns with the 'snippets' column\n",
    "df_result = pd.concat([df['snippet'].reset_index(drop=True), df_split], axis=1)\n",
    "\n",
    "# df_clustered = calculate_cluster(df[:10], cols=\"vector\", n_clusters=n_clusters)\n",
    "df_clustered, clusterer = calculate_cluster(df_result, cols=range(0, 383), n_clusters=n_clusters)\n",
    "\n",
    "df_clustered = pd.concat([df_clustered['cluster_id'], df.reset_index(drop=True)], axis=1) # .reset_index(drop=True)\n",
    "df_clustered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_3d = dimensionality_reduction.reduce_dimensionality_of_project_data(df_clustered, metric=\"cosine\", vector_column_name=\"embedding\")\n",
    "df_dim_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def save_as_json(df):\n",
    "    data = {\"clusters\": []}\n",
    "\n",
    "    # Group the data by cluster_id\n",
    "    grouped = df.groupby(\"cluster_id\")\n",
    "    for cluster_id, group in grouped:\n",
    "        cluster = {\"name\": cluster_id, \"data\": []}\n",
    "        for index, row in group.iterrows():\n",
    "            row_data = {\n",
    "                \"id\": row[\"id\"],\n",
    "                \"tooltip\": row[\"tooltip\"],\n",
    "                \"x\": row[\"x\"],\n",
    "                \"y\": row[\"y\"],\n",
    "                \"z\": row[\"z\"]\n",
    "            }\n",
    "            cluster[\"data\"].append(row_data)\n",
    "        data[\"clusters\"].append(cluster)\n",
    "\n",
    "    with open(\"big_consulting_export-clustered.json\", \"w\") as outfile:\n",
    "        json.dump(data, outfile)\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pyo.init_notebook_mode()\n",
    "\n",
    "filtered_df = df_dim_3d\n",
    "# filtered_df = filtered_df[filtered_df['cluster_id'] != \"31\"]\n",
    "# filtered_df = filtered_df[filtered_df['cluster_id'] != \"-1\"]\n",
    "filtered_df[\"tooltip\"] = filtered_df[\"tooltip\"].str.replace(r\"/n\", \"<br>\")\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(filtered_df,\n",
    "                    x=\"x\", y=\"y\", z=\"z\",\n",
    "                    hover_data=['snippet', \"tooltip\"],\n",
    "                    color=\"cluster_id\",\n",
    "                    width=1200, height=1200\n",
    "                    )\n",
    "fig.show()\n",
    "fig.write_image(f\"{json_file_path.split('.')[0]}.svg\")\n",
    "fig.write_html(f\"{json_file_path.split('.')[0]}.html\", include_plotlyjs=True, full_html=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# https://notebook.community/scikit-learn-contrib/hdbscan/notebooks/Looking%20at%20cluster%20consistency\n",
    "class RankedPoints:\n",
    "    \n",
    "    def __init__(self, points, clusterer, metric='euclidean', selection_method='centroid'):\n",
    "        \"\"\" Rank points in a cluster based on their distance to the cluster centroid/medoid\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        points : array of shape (n_samples, n_features), and must be the same data passed into\n",
    "                 HDBSCAN\n",
    "        \n",
    "        clusterer : Instance of HDBSCAN that has been fit to data\n",
    "        \n",
    "        metric: string or callable, optional (default='euclidean')\n",
    "            The metric to use when calculating distance between points in a cluster and \n",
    "            the cluster centroid/medoid. If metric is a string or callable, it must be one of\n",
    "            the options allowed by scipy.spatial.distance.cdist for its metric parameter.\n",
    "        \n",
    "        selection_method: string, optional (default='centroid')\n",
    "            Method to use to find the weighted cluster center. Allowed options are 'centroid' \n",
    "            and 'medoid'.\n",
    "        \n",
    "        \"\"\"\n",
    "        self.clusterer = clusterer\n",
    "        self.metric = metric\n",
    "        \n",
    "        allowed_methods = ['centroid', 'medoid']\n",
    "        if selection_method not in allowed_methods:\n",
    "            raise ValueError(f'Selection method must be one of {allowed_methods}')\n",
    "        \n",
    "        if selection_method == 'centroid' and metric != 'euclidean':\n",
    "            raise ValueError(f'Metric must be euclidian when using selection_method centroid. '\n",
    "                             f'Current metric is {metric}')\n",
    "        \n",
    "        self.selection_method = selection_method\n",
    "        \n",
    "        self._embedding_cols = [str(i) for i in range(points.shape[1])]\n",
    "        self.embedding_df = pd.DataFrame(points, columns=self._embedding_cols)\n",
    "        self.embedding_df['cluster'] = clusterer.labels_\n",
    "    \n",
    "    def calculate_all_distances_to_center(self):\n",
    "        \"\"\"For each cluster calculate the distance from each point to the centroid/medoid\"\"\"\n",
    "        all_distances = pd.DataFrame()\n",
    "        for label in np.unique(self.embedding_df['cluster']):           \n",
    "            distance_df = self.calculate_distances_for_cluster(label)\n",
    "            all_distances = pd.concat([all_distances, distance_df])\n",
    "        \n",
    "        self.embedding_df = self.embedding_df.merge(all_distances, left_index=True, right_index=True)\n",
    "    \n",
    "    def calculate_distances_for_cluster(self, cluster_id):\n",
    "        \"\"\"For a given cluster_id calculate the distance from each point to the centroid/medoid.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        cluster_id : int\n",
    "            The id of the cluster to compute the distances for. If the cluster id is -1 which\n",
    "            corresponds to the noise point cluster, then this will return a distance of NaN.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        df : A pandas DataFrame containing the distances from each point to the cluster centroid/medoid.\n",
    "             The index of the dataframe corresponds to the index in the original data. \n",
    "\n",
    "        \"\"\"\n",
    "        cluster_of_interest = self.embedding_df[self.embedding_df['cluster'] == cluster_id].copy()\n",
    "        \n",
    "        if cluster_of_interest.empty:\n",
    "            raise ValueError(f'Cluster id {cluster_id} not found')\n",
    "        \n",
    "        # Don't calculate distances for the noise cluster\n",
    "        if cluster_id == -1:\n",
    "            return pd.DataFrame(np.nan, columns=['dist_to_rep_point'], index=cluster_of_interest.index)\n",
    "        \n",
    "        if self.selection_method == 'centroid':\n",
    "            rep_point = self.clusterer.weighted_cluster_centroid(cluster_id)\n",
    "        if self.selection_method == 'medoid':\n",
    "            rep_point = self.clusterer.weighted_cluster_medoid(cluster_id)\n",
    "        \n",
    "        dists = cdist(rep_point.reshape((1,len(self._embedding_cols))), cluster_of_interest[self._embedding_cols].values, metric=self.metric)\n",
    "        return pd.DataFrame(dists[0], columns=['dist_to_rep_point'], index=cluster_of_interest.index)\n",
    "    \n",
    "    def rank_cluster_points_by_distance(self, cluster_id):\n",
    "        \"\"\"For a given cluster return a pandas dataframe of points ranked \n",
    "           by distance to the cluster centroid/medoid\n",
    "        \"\"\"\n",
    "        cluster_of_interest = self.embedding_df[self.embedding_df['cluster'] == cluster_id].copy()\n",
    "        \n",
    "        if cluster_of_interest.empty:\n",
    "            raise ValueError(f'Cluster id {cluster_id} not found')\n",
    "            \n",
    "        if 'dist_to_rep_point' not in self.embedding_df.columns:\n",
    "            distance_df = self.calculate_distances_for_cluster(cluster_id)\n",
    "            cluster_of_interest = cluster_of_interest.merge(distance_df, left_index=True, right_index=True)\n",
    "        \n",
    "        cluster_of_interest.sort_values('dist_to_rep_point', inplace=True)\n",
    "        return cluster_of_interest\n",
    "    \n",
    "    def get_closest_samples_for_cluster(self, cluster_id, n_samples=5):\n",
    "        \"\"\"Get the N closest points to the cluster centroid/medoid\"\"\"\n",
    "        return self.rank_cluster_points_by_distance(cluster_id).head(n_samples)\n",
    "    \n",
    "    def get_furthest_samples_for_cluster(self, cluster_id, n_samples=5):\n",
    "        \"\"\"Get the N points furthest away from the cluster centroid/medoid\"\"\"\n",
    "        return self.rank_cluster_points_by_distance(cluster_id).tail(n_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = df_result[range(0, 383)].to_numpy()\n",
    "examples = RankedPoints(points, clusterer, metric='euclidean', selection_method='medoid')\n",
    "examples.calculate_all_distances_to_center()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dist_to_rep_point</th>\n",
       "      <th>cluster_id</th>\n",
       "      <th>tooltip</th>\n",
       "      <th>score</th>\n",
       "      <th>snippet</th>\n",
       "      <th>id</th>\n",
       "      <th>embedding</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "      <th>final_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2023-04-21) Strategy|Company Info\\n\\nat KPMG ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>at KPMG where he focused on complex financial ...</td>\n",
       "      <td>ID0</td>\n",
       "      <td>[0.06154659017920494, 0.08247778564691544, -0....</td>\n",
       "      <td>3.828489</td>\n",
       "      <td>0.478022</td>\n",
       "      <td>5.029356</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2023-04-21) Leadership\\n\\nLatentView has been...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>LatentView has been recognized as an industry ...</td>\n",
       "      <td>ID1</td>\n",
       "      <td>[-0.017490660771727562, -0.036421071738004684,...</td>\n",
       "      <td>3.319351</td>\n",
       "      <td>2.188711</td>\n",
       "      <td>12.027831</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2023-04-21) \\n\\nIn his last role, Prashant wa...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>In his last role, Prashant was playing the rol...</td>\n",
       "      <td>ID2</td>\n",
       "      <td>[0.07539796829223633, -0.025188380852341652, -...</td>\n",
       "      <td>2.926881</td>\n",
       "      <td>13.724097</td>\n",
       "      <td>5.736260</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2023-04-21) \\n\\nWe're proud of the end result...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>We're proud of the end result of this implemen...</td>\n",
       "      <td>ID3</td>\n",
       "      <td>[-0.030860286206007004, 0.047984544187784195, ...</td>\n",
       "      <td>0.304015</td>\n",
       "      <td>3.750470</td>\n",
       "      <td>12.124339</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.295331</td>\n",
       "      <td>31</td>\n",
       "      <td>(2023-04-21) Market Share Growth\\n\\nWipro cons...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Wipro consolidates presence in foods with acqu...</td>\n",
       "      <td>ID4</td>\n",
       "      <td>[0.08279228955507278, 0.027778802439570427, -0...</td>\n",
       "      <td>3.036174</td>\n",
       "      <td>-3.929799</td>\n",
       "      <td>13.274285</td>\n",
       "      <td>1.295331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13499</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2023-03-29) \\n\\nProfessional services firmAcc...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Professional services firmAccenture has flagge...</td>\n",
       "      <td>ID13499</td>\n",
       "      <td>[-0.005730913951992989, -0.0424995943903923, -...</td>\n",
       "      <td>8.403257</td>\n",
       "      <td>9.505851</td>\n",
       "      <td>2.837073</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13500</th>\n",
       "      <td>1.248475</td>\n",
       "      <td>31</td>\n",
       "      <td>(2023-03-29) \\n\\nAccenture stated that it had ...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>Accenture stated that it had put aside $1.5 bi...</td>\n",
       "      <td>ID13500</td>\n",
       "      <td>[0.03951822593808174, 0.005480567459017038, 0....</td>\n",
       "      <td>3.673986</td>\n",
       "      <td>8.014185</td>\n",
       "      <td>-0.528267</td>\n",
       "      <td>1.560593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13501</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2023-03-29) \\n\\nMeanwhile, Accenture last wee...</td>\n",
       "      <td>0.6</td>\n",
       "      <td>Meanwhile, Accenture last week closed two deals</td>\n",
       "      <td>ID13501</td>\n",
       "      <td>[0.010188382118940353, -0.0003712352190632373,...</td>\n",
       "      <td>7.694917</td>\n",
       "      <td>7.426668</td>\n",
       "      <td>7.825679</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13502</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2023-03-29) \\n\\nS4 Capital was forced to dela...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>S4 Capital was forced to delay its results twi...</td>\n",
       "      <td>ID13502</td>\n",
       "      <td>[-0.037712085992097855, 0.016464950516819954, ...</td>\n",
       "      <td>2.621881</td>\n",
       "      <td>5.651892</td>\n",
       "      <td>1.409518</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13503</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1</td>\n",
       "      <td>(2023-03-29) \\n\\nWith this signing, IBM aims t...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>With this signing, IBM aims to strengthen its ...</td>\n",
       "      <td>ID13503</td>\n",
       "      <td>[0.00866389274597168, 0.06573154032230377, 0.0...</td>\n",
       "      <td>4.904515</td>\n",
       "      <td>7.308236</td>\n",
       "      <td>5.454713</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13504 rows Ã 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       dist_to_rep_point cluster_id  \\\n",
       "0                    NaN         -1   \n",
       "1                    NaN         -1   \n",
       "2                    NaN         -1   \n",
       "3                    NaN         -1   \n",
       "4               1.295331         31   \n",
       "...                  ...        ...   \n",
       "13499                NaN         -1   \n",
       "13500           1.248475         31   \n",
       "13501                NaN         -1   \n",
       "13502                NaN         -1   \n",
       "13503                NaN         -1   \n",
       "\n",
       "                                                 tooltip  score  \\\n",
       "0      (2023-04-21) Strategy|Company Info\\n\\nat KPMG ...    1.0   \n",
       "1      (2023-04-21) Leadership\\n\\nLatentView has been...    1.0   \n",
       "2      (2023-04-21) \\n\\nIn his last role, Prashant wa...    0.8   \n",
       "3      (2023-04-21) \\n\\nWe're proud of the end result...    0.8   \n",
       "4      (2023-04-21) Market Share Growth\\n\\nWipro cons...    1.0   \n",
       "...                                                  ...    ...   \n",
       "13499  (2023-03-29) \\n\\nProfessional services firmAcc...    0.8   \n",
       "13500  (2023-03-29) \\n\\nAccenture stated that it had ...    0.8   \n",
       "13501  (2023-03-29) \\n\\nMeanwhile, Accenture last wee...    0.6   \n",
       "13502  (2023-03-29) \\n\\nS4 Capital was forced to dela...    0.8   \n",
       "13503  (2023-03-29) \\n\\nWith this signing, IBM aims t...    0.8   \n",
       "\n",
       "                                                 snippet       id  \\\n",
       "0      at KPMG where he focused on complex financial ...      ID0   \n",
       "1      LatentView has been recognized as an industry ...      ID1   \n",
       "2      In his last role, Prashant was playing the rol...      ID2   \n",
       "3      We're proud of the end result of this implemen...      ID3   \n",
       "4      Wipro consolidates presence in foods with acqu...      ID4   \n",
       "...                                                  ...      ...   \n",
       "13499  Professional services firmAccenture has flagge...  ID13499   \n",
       "13500  Accenture stated that it had put aside $1.5 bi...  ID13500   \n",
       "13501    Meanwhile, Accenture last week closed two deals  ID13501   \n",
       "13502  S4 Capital was forced to delay its results twi...  ID13502   \n",
       "13503  With this signing, IBM aims to strengthen its ...  ID13503   \n",
       "\n",
       "                                               embedding         x          y  \\\n",
       "0      [0.06154659017920494, 0.08247778564691544, -0....  3.828489   0.478022   \n",
       "1      [-0.017490660771727562, -0.036421071738004684,...  3.319351   2.188711   \n",
       "2      [0.07539796829223633, -0.025188380852341652, -...  2.926881  13.724097   \n",
       "3      [-0.030860286206007004, 0.047984544187784195, ...  0.304015   3.750470   \n",
       "4      [0.08279228955507278, 0.027778802439570427, -0...  3.036174  -3.929799   \n",
       "...                                                  ...       ...        ...   \n",
       "13499  [-0.005730913951992989, -0.0424995943903923, -...  8.403257   9.505851   \n",
       "13500  [0.03951822593808174, 0.005480567459017038, 0....  3.673986   8.014185   \n",
       "13501  [0.010188382118940353, -0.0003712352190632373,...  7.694917   7.426668   \n",
       "13502  [-0.037712085992097855, 0.016464950516819954, ...  2.621881   5.651892   \n",
       "13503  [0.00866389274597168, 0.06573154032230377, 0.0...  4.904515   7.308236   \n",
       "\n",
       "               z  final_score  \n",
       "0       5.029356     1.000000  \n",
       "1      12.027831     1.000000  \n",
       "2       5.736260     0.800000  \n",
       "3      12.124339     0.800000  \n",
       "4      13.274285     1.295331  \n",
       "...          ...          ...  \n",
       "13499   2.837073     0.800000  \n",
       "13500  -0.528267     1.560593  \n",
       "13501   7.825679     0.600000  \n",
       "13502   1.409518     0.800000  \n",
       "13503   5.454713     0.800000  \n",
       "\n",
       "[13504 rows x 11 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_df = examples.embedding_df # [examples.embedding_df['cluster'] != -1]\n",
    "emb_df_result = pd.concat([emb_df['dist_to_rep_point'].reset_index(drop=True), df_dim_3d], axis=1)\n",
    "\n",
    "# emb_df_result_main_cols = emb_df_result.iloc[:, [0, -2, -1]]\n",
    "# emb_df_result_main_cols.to_csv(f'big_consulting_export_clustered-hdbscan-{n_clusters}-leaf.csv', index=True)\n",
    "\n",
    "emb_df_result[\"final_score\"] = emb_df_result[\"dist_to_rep_point\"] / emb_df_result[\"score\"]\n",
    "emb_df_result.loc[emb_df_result[\"dist_to_rep_point\"].isna(), \"final_score\"] = emb_df_result[\"score\"]\n",
    "\n",
    "emb_df_result.to_csv(f'big_consulting_export_clustered-hdbscan-{n_clusters}-leaf.csv', index=True)\n",
    "emb_df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
