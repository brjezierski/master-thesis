{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e37acab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import classification_training_utils\n",
    "importlib.reload(classification_training_utils)\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util\n",
    "from sentence_transformers.datasets import SentenceLabelDataset\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from utils import load_model, replace_nan_with, load_big_consulting_export, callback\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import model_freeze as freeze\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "from classification_training_utils import get_big_consulting_df, collect_classification_labels, get_relevant_classifications, train, filter_relevant_classifications, get_top_values, get_news_df\n",
    "from utils import create_replace_no_tags_embeddings\n",
    "from transformers import EarlyStoppingCallback\n",
    "import math\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d649363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def encode_w_sbert(snippet):\n",
    "#     return model.encode(snippet)\n",
    "\n",
    "# def save_batch(col_name):\n",
    "#     train_temp_df = pd.read_csv(f'{dataset_dir}train.tsv', sep='\\t')\n",
    "#     train_temp_df['embedding'] = train_temp_df.progress_apply(\n",
    "#                     lambda row: encode_w_sbert(row[col_name]), axis=1)\n",
    "#     train_temp_df.to_csv(f'../classification-training-data/{col_name}/train.tsv', sep='\\t')\n",
    "\n",
    "#     test_temp_df = pd.read_csv(f'{dataset_dir}test.tsv', sep='\\t')\n",
    "#     test_temp_df['embedding'] = test_temp_df.progress_apply(\n",
    "#                     lambda row: encode_w_sbert(row[col_name]), axis=1)\n",
    "#     test_temp_df.to_csv(f'../classification-training-data/{col_name}/test.tsv', sep='\\t')\n",
    "\n",
    "#     dev_temp_df = pd.read_csv(f'{dataset_dir}dev.tsv', sep='\\t')\n",
    "#     dev_temp_df['embedding'] = dev_temp_df.progress_apply(\n",
    "#                     lambda row: encode_w_sbert(row[col_name]), axis=1)\n",
    "#     dev_temp_df.to_csv(f'../classification-training-data/{col_name}/dev.tsv', sep='\\t')\n",
    "\n",
    "# save_batch('snippet')\n",
    "# save_batch('replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ba4423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"UNFREEZE_LAYERS\"] = 2\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "params[\"INITIALIZED_MODEL\"] = \"all-MiniLM-L12-v2\" #\"brjezierski/S3BERT\" # \"intfloat/e5-small-v2\"\n",
    "params[\"OCCURENCE_CUTOFF\"] = 2\n",
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"BATCH_SIZE\"] = 32\n",
    "params[\"VAL_DEV_SIZE\"] = 100\n",
    "params['SNIPPET_COLUMN_NAME'] = 'replace' # [\"replace_no_tags\", \"replace\", 'snippet']\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"] # [\"ai\", \"car\", \"consulting\", \"consulting2\"]\n",
    "\n",
    "dataset_dir = \"../classification-training-data/replace/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8de1935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(save_model=False):\n",
    "    model = load_model() if not params[\"INITIALIZED_MODEL\"] else load_model(\n",
    "        model=params[\"INITIALIZED_MODEL\"])\n",
    "    if \"consulting\" in params[\"DATASETS\"]:\n",
    "        big_consulting_df = get_big_consulting_df(params)\n",
    "        big_consulting_df = collect_classification_labels(big_consulting_df)\n",
    "        big_consulting_df = get_top_values(big_consulting_df, params)\n",
    "        big_consulting_all_classifications, big_consulting_relevant_classifications = get_relevant_classifications(big_consulting_df, params)\n",
    "        big_consulting_df, big_consulting_classifications = filter_relevant_classifications(big_consulting_df, big_consulting_all_classifications, big_consulting_relevant_classifications)\n",
    "        params[\"WARMUP_STEPS\"] = int(len(big_consulting_df) * params[\"EPOCHS\"] * 0.1)\n",
    "\n",
    "    if \"ai\" in params[\"DATASETS\"]:\n",
    "        ai_news_df = get_news_df(params, 'ai_news')\n",
    "        ai_news_df = collect_classification_labels(ai_news_df)\n",
    "        if \"NEW_CLASSIFICATIONS\" in params and \"ai\" in params[\"NEW_CLASSIFICATIONS\"]:\n",
    "            with open(params[\"NEW_CLASSIFICATIONS\"][\"ai\"], 'rb') as file:\n",
    "                new_classifications = pickle.load(file)\n",
    "            ai_news_df['top_classification'] = ai_news_df['snippet'].map(new_classifications)\n",
    "        else:\n",
    "            ai_news_df = get_top_values(ai_news_df, params)\n",
    "        ai_news_all_classifications, ai_news_relevant_classifications = get_relevant_classifications(ai_news_df, params)\n",
    "        ai_news_df, ai_news_classifications = filter_relevant_classifications(ai_news_df, ai_news_all_classifications, ai_news_relevant_classifications)\n",
    "        params[\"WARMUP_STEPS\"] = int(len(ai_news_df) * params[\"EPOCHS\"] * 0.1)\n",
    "\n",
    "    if \"car\" in params[\"DATASETS\"]:\n",
    "        car_news_df = get_news_df(params, 'car_news')\n",
    "        # How about duplicating car news dataset?\n",
    "        car_news_df = collect_classification_labels(car_news_df)\n",
    "        if \"NEW_CLASSIFICATIONS\" in params and \"car\" in params[\"NEW_CLASSIFICATIONS\"]:\n",
    "            with open(params[\"NEW_CLASSIFICATIONS\"][\"car\"], 'rb') as file:\n",
    "                new_classifications = pickle.load(file)\n",
    "            car_news_df['top_classification'] = car_news_df['snippet'].map(new_classifications)\n",
    "        else:\n",
    "            car_news_df = get_top_values(car_news_df, params)\n",
    "        car_news_all_classifications, car_news_relevant_classifications = get_relevant_classifications(car_news_df, params)\n",
    "        car_news_df, car_news_classifications = filter_relevant_classifications(car_news_df, car_news_all_classifications, car_news_relevant_classifications)\n",
    "        print(type(car_news_classifications))\n",
    "        params[\"WARMUP_STEPS\"] = int(len(car_news_df) * params[\"EPOCHS\"] * 0.1)\n",
    "\n",
    "    embeddings_prefix = '../glanos-data/embeddings/'\n",
    "\n",
    "    if params['SNIPPET_COLUMN_NAME'] == \"replace_no_tags\":\n",
    "\n",
    "        if \"ai\" in params[\"DATASETS\"]:\n",
    "            with open(f'{embeddings_prefix}ai_news_replace_no_tags.pickle', 'rb') as f:\n",
    "                ai_news_no_tags_embeddings = pickle.load(f)\n",
    "            ai_news_df = create_replace_no_tags_embeddings(ai_news_df, ai_news_no_tags_embeddings)\n",
    "            ai_news_df['embedding'] = ai_news_df['replace_no_tags'].map(ai_news_no_tags_embeddings)\n",
    "\n",
    "        if \"car\" in params[\"DATASETS\"]:\n",
    "            with open(f'{embeddings_prefix}car_news_replace_no_tags.pickle', 'rb') as f:\n",
    "                car_news_no_tags_embeddings = pickle.load(f)\n",
    "            car_news_df = create_replace_no_tags_embeddings(car_news_df, car_news_no_tags_embeddings)\n",
    "            car_news_df['embedding'] = car_news_df['replace_no_tags'].map(car_news_no_tags_embeddings)\n",
    "\n",
    "        if \"consulting\" in params[\"DATASETS\"]:\n",
    "            with open(f'{embeddings_prefix}big_consulting_2_replace_no_tags.pickle', 'rb') as f:\n",
    "                consulting_no_tags_embeddings = pickle.load(f)\n",
    "            big_consulting_df = create_replace_no_tags_embeddings(big_consulting_df, consulting_no_tags_embeddings)\n",
    "            big_consulting_df['embedding'] = big_consulting_df['replace_no_tags'].map(consulting_no_tags_embeddings)\n",
    "\n",
    "    classifications_path =  \"../classification-training-data/classifications.pkl\"\n",
    "\n",
    "    with open(classifications_path, 'rb') as file:\n",
    "        classifications = pickle.load(file)\n",
    "\n",
    "    training_datasets = []\n",
    "    classification_lists = []\n",
    "    if \"consulting\" in params[\"DATASETS\"]:\n",
    "        training_datasets.append(big_consulting_df)\n",
    "        classification_lists.append(big_consulting_classifications)\n",
    "    if \"ai\" in params[\"DATASETS\"]:\n",
    "        ai_news_df.drop(columns=['embedding'])\n",
    "        if params[\"DATASETS\"].count(\"ai\") == 2:\n",
    "            ai_news_df = pd.concat([ai_news_df, ai_news_df], axis=0).reset_index(drop=True) \n",
    "        training_datasets.append(ai_news_df)\n",
    "        classification_lists.append(ai_news_classifications)\n",
    "    if \"car\" in params[\"DATASETS\"]:\n",
    "        car_news_df.drop(columns=['embedding'])\n",
    "        if params[\"DATASETS\"].count(\"car\") == 2:\n",
    "            car_news_df = pd.concat([car_news_df, car_news_df], axis=0).reset_index(drop=True) \n",
    "        training_datasets.append(car_news_df)\n",
    "        classification_lists.append(car_news_classifications)\n",
    "\n",
    "#     classification_lists = [classifications] * len(training_datasets)            \n",
    "\n",
    "    print(\"Using datasets:\", params[\"DATASETS\"])\n",
    "\n",
    "    model_fit, test_evaluator, model = train(training_datasets, classification_lists, params, dataset_dir, model, save_model=save_model)\n",
    "\n",
    "    print('Score', model.evaluate(test_evaluator))\n",
    "\n",
    "    frozen_model = load_model() if not params[\"INITIALIZED_MODEL\"] else load_model(\n",
    "        model=params[\"INITIALIZED_MODEL\"])\n",
    "    print('Baseline', frozen_model.evaluate(test_evaluator))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b4a9243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params[\"USE_REPLACE_DATA\"] = True\n",
    "# big_consulting_replace_df = get_big_consulting_df(params)\n",
    "# params[\"USE_REPLACE_DATA\"] = False\n",
    "# big_consulting_no_replace_df = get_big_consulting_df(params)\n",
    "# df = pd.concat([big_consulting_replace_df, big_consulting_no_replace_df, ai_news_df, car_news_df], axis=0).reset_index(drop=True) #big_consulting_df, ai_news_df, car_news_df\n",
    "# df = collect_classification_labels(df, verbose=False)\n",
    "# df = get_top_values(df, params)\n",
    "# all_classifications, relevant_classifications = get_relevant_classifications(df, params)\n",
    "# df, classifications = filter_relevant_classifications(df, all_classifications, relevant_classifications)\n",
    "# params[\"WARMUP_STEPS\"] = int(len(df) * params[\"EPOCHS\"] * 0.1)  # 10% of train data\n",
    "\n",
    "classifications_path =  \"../classification-training-data/classifications.pkl\"\n",
    "# with open(\"../classification-training-data/classifications.pkl\", 'wb') as file:\n",
    "#     pickle.dump(classifications, file)\n",
    "    \n",
    "with open(classifications_path, 'rb') as file:\n",
    "    classifications = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "acc5702a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Accuracy Cosine Distance\n",
    "# baseline - 0.77\n",
    "# e=2 Using original data - 0.83\n",
    "# BEST e=2 Using original data, unfreezing 2 last layers, only including words that occur at least 2 times - 0.93\n",
    "# e=2 Using original data, unfreezing 2 last layers, only including words that occur at least 3 times - 0.93\n",
    "# e=2 Using original data, unfreezing 2 last layers, only including words that occur at least 4 times - 0.92\n",
    "# e=1 Using original data, unfreezing 2 last layers, starting with brjezierski/S3BERT, only including words that occur at least 2 times - 0.89\n",
    "# e=2 Using original data, unfreezing 2 last layers, starting with brjezierski/S3BERT, only including words that occur at least 2 times - 0.92\n",
    "\n",
    "# e=2 Using original and replacement data, unfreezing 2 last layers, excluding Entity and Other labels, starting with brjezierski/S3BERT, only including words that occur at least 2 times - 0.91\n",
    "# e=2 Using original and replacement data, unfreezing 2 last layers, starting with brjezierski/S3BERT, only including words that occur at least 2 times - 0.88\n",
    "# e=1 Using original and replacement data, unfreezing 2 last layers, excluding Entity and Other labels, starting with brjezierski/S3BERT, only including words that occur at least 2 times - 0.92\n",
    "\n",
    "# e=2 Using original data, unfreezing 2 last layers, starting with intfloat/e5-small-v2, only including words that occur at least 2 times - 0.91\n",
    "# e=2 Using original and replacement data, unfreezing 2 last layers, excluding Entity and Other labels, starting with intfloat/e5-small-v2, only including words that occur at least 2 times - 0.89\n",
    "\n",
    "# e=2 Using replacement data, creating a new train-dev-test split, starting with brjezierski/S3BERT, only including words that occur at least 2 times - 0.96, 0.88 (baseline)\n",
    "# e=1 Using replacement data, creating a new train-dev-test split, starting with brjezierski/S3BERT, only including words that occur at least 2 times - 0.94\n",
    "# e=2 Using replacement data, creating a new train-dev-test split, excluding Entity and Other labels, starting with brjezierski/S3BERT, only including words that occur at least 2 times - 0.94\n",
    "# e=1 Using replacement data, creating a new train-dev-test split, excluding Entity and Other labels, starting with brjezierski/S3BERT, only including words that occur at least 2 times - 0.96\n",
    "\n",
    "# e=1 Using replacement data, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size 67325+38639\n",
    "\n",
    "\n",
    "# Thesis testing\n",
    "# From normal embeddings on combined ai+car /w new test test: 78 vs. 80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b292f13",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5aad903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex4\n",
      "<class 'dict'>\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m params[\u001b[39m'\u001b[39m\u001b[39mSNIPPET_COLUMN_NAME\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msnippet\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     17\u001b[0m dataset_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../classification-training-data/snippet_new/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 18\u001b[0m run_pipeline()\n\u001b[1;32m     19\u001b[0m \u001b[39m# Score 0.7604166666666666\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# Baseline 0.7569444444444444\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEx5\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 89\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(save_model)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[39m#     classification_lists = [classifications] * len(training_datasets)            \u001b[39;00m\n\u001b[1;32m     87\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mUsing datasets:\u001b[39m\u001b[39m\"\u001b[39m, params[\u001b[39m\"\u001b[39m\u001b[39mDATASETS\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m---> 89\u001b[0m     model_fit, test_evaluator, model \u001b[39m=\u001b[39m train(training_datasets, classification_lists, params, dataset_dir, model, save_model\u001b[39m=\u001b[39;49msave_model)\n\u001b[1;32m     91\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mScore\u001b[39m\u001b[39m'\u001b[39m, model\u001b[39m.\u001b[39mevaluate(test_evaluator))\n\u001b[1;32m     93\u001b[0m     frozen_model \u001b[39m=\u001b[39m load_model() \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m params[\u001b[39m\"\u001b[39m\u001b[39mINITIALIZED_MODEL\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39melse\u001b[39;00m load_model(\n\u001b[1;32m     94\u001b[0m         model\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mINITIALIZED_MODEL\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/master-thesis/notebooks/classification_training_utils.py:452\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(classified_df, classifications, params, dataset_dir, sbert_model, save_model)\u001b[0m\n\u001b[1;32m    450\u001b[0m     train_objectives \u001b[39m=\u001b[39m []\n\u001b[1;32m    451\u001b[0m     \u001b[39mfor\u001b[39;00m sub_classified_df, sub_classifications \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(classified_df, classifications):\n\u001b[0;32m--> 452\u001b[0m         train_objective, dev_evaluator, test_evaluator \u001b[39m=\u001b[39m prepare_for_training(\n\u001b[1;32m    453\u001b[0m             sub_classified_df, sub_classifications, params, dataset_dir, sbert_model)\n\u001b[1;32m    454\u001b[0m         train_objectives\u001b[39m.\u001b[39mextend(train_objective)\n\u001b[1;32m    455\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Desktop/master-thesis/notebooks/classification_training_utils.py:388\u001b[0m, in \u001b[0;36mprepare_for_training\u001b[0;34m(classified_df, classifications, params, dataset_dir, sbert_model)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     freeze\u001b[39m.\u001b[39mfreeze_all_layers(sbert_model)\n\u001b[0;32m--> 388\u001b[0m train_set, dev_set, test_set \u001b[39m=\u001b[39m get_dataset(\n\u001b[1;32m    389\u001b[0m     classified_df, params, classifications, create_new_split, dataset_dir, val_dev_size\u001b[39m=\u001b[39;49mparams[\u001b[39m\"\u001b[39;49m\u001b[39mVAL_DEV_SIZE\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    391\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39me=\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m\n\u001b[1;32m    392\u001b[0m       (\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, embeddings from \u001b[39m\u001b[39m{\u001b[39;00msnippet_column_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m\n\u001b[1;32m    393\u001b[0m       (\u001b[39m\"\u001b[39m\u001b[39m, creating a new train-dev-test split\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m create_new_split \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m+\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    399\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    400\u001b[0m       )\n\u001b[1;32m    401\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining data size\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(train_set))\n",
      "File \u001b[0;32m~/Desktop/master-thesis/notebooks/classification_training_utils.py:306\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(df, params, label_map, create_new_split, dataset_dir, val_dev_size)\u001b[0m\n\u001b[1;32m    304\u001b[0m guid \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m create_new_split:\n\u001b[0;32m--> 306\u001b[0m     train_df, dev_df, test_df \u001b[39m=\u001b[39m get_train_dev_test_split(\n\u001b[1;32m    307\u001b[0m         df, dataset_dir, val_dev_size)\n\u001b[1;32m    308\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     train_df, dev_df, test_df \u001b[39m=\u001b[39m read_train_dev_test_files(\n\u001b[1;32m    310\u001b[0m         df, dataset_dir)\n",
      "File \u001b[0;32m~/Desktop/master-thesis/notebooks/classification_training_utils.py:256\u001b[0m, in \u001b[0;36mget_train_dev_test_split\u001b[0;34m(df, dataset_dir, val_dev_size)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m# train_df.to_csv(f'{dataset_dir}train.tsv', sep='\\t')\u001b[39;00m\n\u001b[1;32m    255\u001b[0m dev_df\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdataset_dir\u001b[39m}\u001b[39;00m\u001b[39mdev.tsv\u001b[39m\u001b[39m'\u001b[39m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 256\u001b[0m test_df\u001b[39m.\u001b[39;49mto_csv(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m{\u001b[39;49;00mdataset_dir\u001b[39m}\u001b[39;49;00m\u001b[39mtest.tsv\u001b[39;49m\u001b[39m'\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\t\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    258\u001b[0m \u001b[39mreturn\u001b[39;00m train_df, dev_df, test_df\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/core/generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[0;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[1;32m   3761\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[1;32m   3763\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[1;32m   3764\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[1;32m   3765\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3769\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[1;32m   3770\u001b[0m )\n\u001b[0;32m-> 3772\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[1;32m   3773\u001b[0m     path_or_buf,\n\u001b[1;32m   3774\u001b[0m     lineterminator\u001b[39m=\u001b[39;49mlineterminator,\n\u001b[1;32m   3775\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[1;32m   3776\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m   3777\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m   3778\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m   3779\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[1;32m   3780\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[1;32m   3781\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[1;32m   3782\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[1;32m   3783\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[1;32m   3784\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[1;32m   3785\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[1;32m   3786\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[1;32m   3787\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[1;32m   3788\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   3789\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/io/formats/format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[0;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m   1167\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[1;32m   1168\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[1;32m   1169\u001b[0m     lineterminator\u001b[39m=\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1184\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[1;32m   1185\u001b[0m )\n\u001b[0;32m-> 1186\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[1;32m   1189\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/io/formats/csvs.py:259\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m    241\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilepath_or_buffer,\n\u001b[1;32m    242\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[1;32m    248\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[1;32m    250\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    251\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlineterminator,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    256\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[1;32m    257\u001b[0m     )\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save()\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/io/formats/csvs.py:264\u001b[0m, in \u001b[0;36mCSVFormatter._save\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_need_to_save_header:\n\u001b[1;32m    263\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_header()\n\u001b[0;32m--> 264\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_body()\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/io/formats/csvs.py:302\u001b[0m, in \u001b[0;36mCSVFormatter._save_body\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m start_i \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m end_i:\n\u001b[1;32m    301\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_chunk(start_i, end_i)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/io/formats/csvs.py:313\u001b[0m, in \u001b[0;36mCSVFormatter._save_chunk\u001b[0;34m(self, start_i, end_i)\u001b[0m\n\u001b[1;32m    310\u001b[0m data \u001b[39m=\u001b[39m [res\u001b[39m.\u001b[39miget_values(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(res\u001b[39m.\u001b[39mitems))]\n\u001b[1;32m    312\u001b[0m ix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_index[slicer]\u001b[39m.\u001b[39m_format_native_types(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_number_format)\n\u001b[0;32m--> 313\u001b[0m libwriters\u001b[39m.\u001b[39;49mwrite_csv_rows(\n\u001b[1;32m    314\u001b[0m     data,\n\u001b[1;32m    315\u001b[0m     ix,\n\u001b[1;32m    316\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnlevels,\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcols,\n\u001b[1;32m    318\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwriter,\n\u001b[1;32m    319\u001b[0m )\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/pandas/_libs/writers.pyx:55\u001b[0m, in \u001b[0;36mpandas._libs.writers.write_csv_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/numpy/core/arrayprint.py:1592\u001b[0m, in \u001b[0;36m_array_str_implementation\u001b[0;34m(a, max_line_width, precision, suppress_small, array2string)\u001b[0m\n\u001b[1;32m   1586\u001b[0m \u001b[39mif\u001b[39;00m a\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m ():\n\u001b[1;32m   1587\u001b[0m     \u001b[39m# obtain a scalar and call str on it, avoiding problems for subclasses\u001b[39;00m\n\u001b[1;32m   1588\u001b[0m     \u001b[39m# for which indexing with () returns a 0d instead of a scalar by using\u001b[39;00m\n\u001b[1;32m   1589\u001b[0m     \u001b[39m# ndarray's getindex. Also guard against recursive 0d object arrays.\u001b[39;00m\n\u001b[1;32m   1590\u001b[0m     \u001b[39mreturn\u001b[39;00m _guarded_repr_or_str(np\u001b[39m.\u001b[39mndarray\u001b[39m.\u001b[39m\u001b[39m__getitem__\u001b[39m(a, ()))\n\u001b[0;32m-> 1592\u001b[0m \u001b[39mreturn\u001b[39;00m array2string(a, max_line_width, precision, suppress_small, \u001b[39m'\u001b[39;49m\u001b[39m \u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/numpy/core/arrayprint.py:736\u001b[0m, in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, legacy)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[39mif\u001b[39;00m a\u001b[39m.\u001b[39msize \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    734\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m[]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 736\u001b[0m \u001b[39mreturn\u001b[39;00m _array2string(a, options, separator, prefix)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/numpy/core/arrayprint.py:513\u001b[0m, in \u001b[0;36m_recursive_guard.<locals>.decorating_function.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    511\u001b[0m repr_running\u001b[39m.\u001b[39madd(key)\n\u001b[1;32m    512\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 513\u001b[0m     \u001b[39mreturn\u001b[39;00m f(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    514\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    515\u001b[0m     repr_running\u001b[39m.\u001b[39mdiscard(key)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/numpy/core/arrayprint.py:546\u001b[0m, in \u001b[0;36m_array2string\u001b[0;34m(a, options, separator, prefix)\u001b[0m\n\u001b[1;32m    543\u001b[0m \u001b[39m# skip over array(\u001b[39;00m\n\u001b[1;32m    544\u001b[0m next_line_prefix \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39mlen\u001b[39m(prefix)\n\u001b[0;32m--> 546\u001b[0m lst \u001b[39m=\u001b[39m _formatArray(a, format_function, options[\u001b[39m'\u001b[39;49m\u001b[39mlinewidth\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    547\u001b[0m                    next_line_prefix, separator, options[\u001b[39m'\u001b[39;49m\u001b[39medgeitems\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    548\u001b[0m                    summary_insert, options[\u001b[39m'\u001b[39;49m\u001b[39mlegacy\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    549\u001b[0m \u001b[39mreturn\u001b[39;00m lst\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/numpy/core/arrayprint.py:889\u001b[0m, in \u001b[0;36m_formatArray\u001b[0;34m(a, format_function, line_width, next_line_prefix, separator, edge_items, summary_insert, legacy)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[39mreturn\u001b[39;00m s\n\u001b[1;32m    887\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    888\u001b[0m     \u001b[39m# invoke the recursive part with an initial index and prefix\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[39mreturn\u001b[39;00m recurser(index\u001b[39m=\u001b[39;49m(),\n\u001b[1;32m    890\u001b[0m                     hanging_indent\u001b[39m=\u001b[39;49mnext_line_prefix,\n\u001b[1;32m    891\u001b[0m                     curr_width\u001b[39m=\u001b[39;49mline_width)\n\u001b[1;32m    892\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    893\u001b[0m     \u001b[39m# recursive closures have a cyclic reference to themselves, which\u001b[39;00m\n\u001b[1;32m    894\u001b[0m     \u001b[39m# requires gc to collect (gh-10620). To avoid this problem, for\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     \u001b[39m# performance and PyPy friendliness, we break the cycle:\u001b[39;00m\n\u001b[1;32m    896\u001b[0m     recurser \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/numpy/core/arrayprint.py:846\u001b[0m, in \u001b[0;36m_formatArray.<locals>.recurser\u001b[0;34m(index, hanging_indent, curr_width)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(trailing_items, \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m    845\u001b[0m     word \u001b[39m=\u001b[39m recurser(index \u001b[39m+\u001b[39m (\u001b[39m-\u001b[39mi,), next_hanging_indent, next_width)\n\u001b[0;32m--> 846\u001b[0m     s, line \u001b[39m=\u001b[39m _extendLine_pretty(\n\u001b[1;32m    847\u001b[0m         s, line, word, elem_width, hanging_indent, legacy)\n\u001b[1;32m    848\u001b[0m     line \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m separator\n\u001b[1;32m    850\u001b[0m \u001b[39mif\u001b[39;00m legacy \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m113\u001b[39m:\n\u001b[1;32m    851\u001b[0m     \u001b[39m# width of the separator is not considered on 1.13\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/numpy/core/arrayprint.py:759\u001b[0m, in \u001b[0;36m_extendLine_pretty\u001b[0;34m(s, line, word, line_width, next_line_prefix, legacy)\u001b[0m\n\u001b[1;32m    757\u001b[0m words \u001b[39m=\u001b[39m word\u001b[39m.\u001b[39msplitlines()\n\u001b[1;32m    758\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(words) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m legacy \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m113\u001b[39m:\n\u001b[0;32m--> 759\u001b[0m     \u001b[39mreturn\u001b[39;00m _extendLine(s, line, word, line_width, next_line_prefix, legacy)\n\u001b[1;32m    761\u001b[0m max_word_length \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(\u001b[39mlen\u001b[39m(word) \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m words)\n\u001b[1;32m    762\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mlen\u001b[39m(line) \u001b[39m+\u001b[39m max_word_length \u001b[39m>\u001b[39m line_width \u001b[39mand\u001b[39;00m\n\u001b[1;32m    763\u001b[0m         \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m \u001b[39mlen\u001b[39m(next_line_prefix)):\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/numpy/core/arrayprint.py:749\u001b[0m, in \u001b[0;36m_extendLine\u001b[0;34m(s, line, word, line_width, next_line_prefix, legacy)\u001b[0m\n\u001b[1;32m    747\u001b[0m     s \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mrstrip() \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    748\u001b[0m     line \u001b[39m=\u001b[39m next_line_prefix\n\u001b[0;32m--> 749\u001b[0m line \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m word\n\u001b[1;32m    750\u001b[0m \u001b[39mreturn\u001b[39;00m s, line\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# params[\"CREATE_NEW_SPLIT\"] = False\n",
    "# print('Ex2')\n",
    "# params['SNIPPET_COLUMN_NAME'] = 'replace'\n",
    "# dataset_dir = \"../classification-training-data/replace/\"\n",
    "# model1 = run_pipeline()\n",
    "\n",
    "# print('Ex3')\n",
    "# params['SNIPPET_COLUMN_NAME'] = 'replace_no_tags'\n",
    "# dataset_dir = \"../classification-training-data/replace_no_tags/\"\n",
    "# model2 = run_pipeline()\n",
    "\n",
    "params[\"VAL_DEV_SIZE\"] = 400\n",
    "params[\"CREATE_NEW_SPLIT\"] = True\n",
    "\n",
    "print('Ex4')\n",
    "params['SNIPPET_COLUMN_NAME'] = 'snippet'\n",
    "dataset_dir = \"../classification-training-data/snippet_new/\"\n",
    "run_pipeline()\n",
    "# Score 0.7604166666666666\n",
    "# Baseline 0.7569444444444444\n",
    "\n",
    "print('Ex5')\n",
    "params['SNIPPET_COLUMN_NAME'] = 'replace'\n",
    "dataset_dir = \"../classification-training-data/replace_new/\"\n",
    "run_pipeline()\n",
    "# Score 0.7927631578947368\n",
    "# Baseline 0.7269736842105263\n",
    "\n",
    "print('Ex6')\n",
    "params['SNIPPET_COLUMN_NAME'] = 'replace_no_tags'\n",
    "dataset_dir = \"../classification-training-data/replace_no_tags_new/\"\n",
    "run_pipeline()\n",
    "# Score 0.8125\n",
    "# Baseline 0.7927631578947368\n",
    "\n",
    "\n",
    "params[\"EPOCHS\"] = 2\n",
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "\n",
    "\n",
    "print('Ex7')\n",
    "params['SNIPPET_COLUMN_NAME'] = 'snippet'\n",
    "dataset_dir = \"../classification-training-data/snippet_new/\"\n",
    "run_pipeline()\n",
    "# Score 0.7708333333333334\n",
    "# Baseline 0.7569444444444444\n",
    "\n",
    "print('Ex8')\n",
    "params['SNIPPET_COLUMN_NAME'] = 'replace'\n",
    "dataset_dir = \"../classification-training-data/replace_new/\"\n",
    "run_pipeline()\n",
    "# Score 0.7894736842105263\n",
    "# Baseline 0.7269736842105263\n",
    "\n",
    "print('Ex9')\n",
    "params['SNIPPET_COLUMN_NAME'] = 'replace_no_tags'\n",
    "dataset_dir = \"../classification-training-data/replace_no_tags_new/\"\n",
    "run_pipeline()\n",
    "# Score 0.7730263157894737\n",
    "# Baseline 0.7927631578947368"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0403f0c",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89629ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex2.1\n",
      "car_news_df 38834\n",
      "car_news_df 77668\n",
      "Using datasets: ['ai', 'car', 'car']\n",
      "Training with multiple objectives\n",
      "Creating new dev and test triplets\n",
      "e=1, embeddings from snippet, creating a new train-dev-test split, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size67312\n",
      "\n",
      "Training data size 67312\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Creating new dev and test triplets\n",
      "e=1, embeddings from snippet, creating a new train-dev-test split, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size77468\n",
      "\n",
      "Training data size 77468\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e838bf61224c4395c8f081faff4ea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ccd081e4ec4313b1796d24890f12f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.79\n",
      "Score 0.87\n",
      "Baseline 0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Ex2.1\")\n",
    "params[\"CREATE_NEW_SPLIT\"] = True\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"EPOCHS\"] = 1\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline(save_model=True)\n",
    "# Best\n",
    "# Score 0.87\n",
    "# Baseline 0.79\n",
    "\n",
    "print(\"Ex2.2\")\n",
    "params[\"CREATE_NEW_SPLIT\"] = True\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'replace'\n",
    "params[\"EPOCHS\"] = 1\n",
    "dataset_dir = \"../classification-training-data/replace_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.79\n",
    "# Baseline 0.82\n",
    "\n",
    "print(\"Ex2.3\")\n",
    "params[\"CREATE_NEW_SPLIT\"] = True\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'replace_no_tags'\n",
    "params[\"EPOCHS\"] = 1\n",
    "dataset_dir = \"../classification-training-data/replace_no_tags_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.8\n",
    "# Baseline 0.77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c6843c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex2.4\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=2, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size67312\n",
      "\n",
      "Training data size 67312\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=2, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size38634\n",
      "\n",
      "Training data size 38634\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7310f3b6fe24360aa9eb59f606f4568",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdb0bf7453b474ab2b909ca48f2abac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e60621ffee742ccb9bc9d946044c858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 1, step -1: 0.76\n",
      "Score 0.81\n",
      "Baseline 0.72\n",
      "Ex2.5\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=2, embeddings from replace, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size67314\n",
      "\n",
      "Training data size 67314\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=2, embeddings from replace, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size38634\n",
      "\n",
      "Training data size 38634\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb173d037b2c4ca69c3641a896a9e33c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b548b5b4234edc97ac24089fa372db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148f7fd6053346f79a9d3d60d7254cdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 1, step -1: 0.82\n",
      "Score 0.8\n",
      "Baseline 0.82\n",
      "Ex2.6\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=2, embeddings from replace_no_tags, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size67314\n",
      "\n",
      "Training data size 67314\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=2, embeddings from replace_no_tags, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size38634\n",
      "\n",
      "Training data size 38634\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d704733fe5d74d1fbd1fcae573ca322f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0efc901a2774410fa751f27e2b2ba009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2b620e48ef478fb51e4624ed5bf85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 1, step -1: 0.78\n",
      "Score 0.81\n",
      "Baseline 0.77\n"
     ]
    }
   ],
   "source": [
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"EPOCHS\"] = 2\n",
    "\n",
    "print(\"Ex2.4\")\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.81\n",
    "# Baseline 0.72\n",
    "\n",
    "print(\"Ex2.5\")\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'replace'\n",
    "dataset_dir = \"../classification-training-data/replace_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.8\n",
    "# Baseline 0.82\n",
    "\n",
    "print(\"Ex2.6\")\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'replace_no_tags'\n",
    "dataset_dir = \"../classification-training-data/replace_no_tags_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.81\n",
    "# Baseline 0.77\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f351bb4",
   "metadata": {},
   "source": [
    "# Experiment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9adbbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex2.6\n",
      "car_news_df 38834\n",
      "car_news_df 77668\n",
      "Using datasets: ['ai', 'car', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size67329\n",
      "\n",
      "Training data size 67329\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size77302\n",
      "\n",
      "Training data size 77302\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd16f8d613b441d4abf3ca32f3c60cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfec88e7204c4d93a4a14331b249922c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/2104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.8\n",
      "Score 0.86\n",
      "Baseline 0.79\n"
     ]
    }
   ],
   "source": [
    "# two times the dataset\n",
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\", \"car\"]\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"UNFREEZE_LAYERS\"] = 2\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "params[\"OCCURENCE_CUTOFF\"] = 2\n",
    "params[\"INITIALIZED_MODEL\"] = \"all-MiniLM-L12-v2\"\n",
    "\n",
    "print(\"Ex2.6\")\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.86\n",
    "# Baseline 0.79\n",
    "# Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff2e3aa",
   "metadata": {},
   "source": [
    "# Experiment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efcdd5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex4.8\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 7 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 2 times, training data size 67329\n",
      "\n",
      "Training data size 67329\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 7 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 2 times, training data size 38651\n",
      "\n",
      "Training data size 38651\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f058d346214d39a58b7fa53b892220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0373add42fe04349aef30b48e3379e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.84\n",
      "Score 0.87\n",
      "Baseline 0.79\n",
      "Ex4.9\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 8 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 2 times, training data size 67329\n",
      "\n",
      "Training data size 67329\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 8 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 2 times, training data size 38651\n",
      "\n",
      "Training data size 38651\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f053efa5384313ac6252a1c9eb68da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fde326255e1948d7b4d1cfb62ed8af1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.87\n",
      "Score 0.9\n",
      "Baseline 0.79\n",
      "Ex4.10\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 9 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 2 times, training data size 67329\n",
      "\n",
      "Training data size 67329\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 9 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 2 times, training data size 38651\n",
      "\n",
      "Training data size 38651\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33eb6728248d4c4b9b2a0908e12c2147",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe406387bdca4d35bd36d57c93f0c081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.89\n",
      "Score 0.89\n",
      "Baseline 0.79\n",
      "Ex4.11\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 10 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 2 times, training data size 67329\n",
      "\n",
      "Training data size 67329\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 10 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 2 times, training data size 38651\n",
      "\n",
      "Training data size 38651\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d64c53828d314ba2ac28748c28c54370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f62e618c233741aba9f4194aca789c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.9\n",
      "Score 0.87\n",
      "Baseline 0.79\n"
     ]
    }
   ],
   "source": [
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"]\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "params[\"OCCURENCE_CUTOFF\"] = 2\n",
    "params[\"INITIALIZED_MODEL\"] = \"all-MiniLM-L12-v2\"\n",
    "\n",
    "\n",
    "# print(\"Ex4.2\")\n",
    "# params[\"UNFREEZE_LAYERS\"] = 1\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.81\n",
    "# # Baseline 0.79\n",
    "\n",
    "\n",
    "# print(\"Ex4.3\")\n",
    "# params[\"UNFREEZE_LAYERS\"] = 2\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.85\n",
    "# # Baseline 0.79\n",
    "\n",
    "\n",
    "# print(\"Ex4.4\")\n",
    "# params[\"UNFREEZE_LAYERS\"] = 3\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.86\n",
    "# # Baseline 0.79\n",
    "\n",
    "\n",
    "# print(\"Ex4.5\")\n",
    "# params[\"UNFREEZE_LAYERS\"] = 4\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.87\n",
    "# # Baseline 0.79\n",
    "\n",
    "\n",
    "# print(\"Ex4.6\")\n",
    "# params[\"UNFREEZE_LAYERS\"] = 5\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.92\n",
    "# # Baseline 0.79\n",
    "\n",
    "# print(\"Ex4.7\")\n",
    "# params[\"UNFREEZE_LAYERS\"] = 6\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.92\n",
    "# # Baseline 0.79\n",
    "\n",
    "\n",
    "print(\"Ex4.8\")\n",
    "params[\"UNFREEZE_LAYERS\"] = 7\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.87\n",
    "# Baseline 0.79\n",
    "\n",
    "print(\"Ex4.9\")\n",
    "params[\"UNFREEZE_LAYERS\"] = 8\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.9\n",
    "# Baseline 0.79\n",
    "\n",
    "print(\"Ex4.10\")\n",
    "params[\"UNFREEZE_LAYERS\"] = 9\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.89\n",
    "# Baseline 0.79\n",
    "\n",
    "print(\"Ex4.11\")\n",
    "params[\"UNFREEZE_LAYERS\"] = 10\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.87\n",
    "# Baseline 0.79"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f68364",
   "metadata": {},
   "source": [
    "# Experiment 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b15a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex5.1\n",
      "car_news_df 38834\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size67329\n",
      "\n",
      "Training data size 67329\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, only including words that occur at least 2 times, training data size38651\n",
      "\n",
      "Training data size 38651\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05956f02b3674e6c8b2b7b13b5b07762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ea5e2450e444f689652a5838e93edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.81\n",
      "Score 0.85\n",
      "Baseline 0.79\n",
      "Ex5.2\n",
      "car_news_df 177800\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, only including words that occur at least 2 times, training data size262127\n",
      "\n",
      "Training data size 262127\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, only including words that occur at least 2 times, training data size177617\n",
      "\n",
      "Training data size 177617\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89b1a7816b6345b2b180ef5a3bfa3a84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c81ab4a3ed24561a28638b2e5ed478c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/5550 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.84\n",
      "Score 0.8\n",
      "Baseline 0.79\n"
     ]
    }
   ],
   "source": [
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"]\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"UNFREEZE_LAYERS\"] = 2\n",
    "params[\"OCCURENCE_CUTOFF\"] = 2\n",
    "params[\"INITIALIZED_MODEL\"] = \"all-MiniLM-L12-v2\"\n",
    "\n",
    "print(\"Ex5.1\")\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.85\n",
    "# Baseline 0.79\n",
    "\n",
    "print(\"Ex5.2\")\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = False\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.8\n",
    "# Baseline 0.79"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8d2694",
   "metadata": {},
   "source": [
    "# Experiment 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a212c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex6.9\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 8 times, training data size 67227\n",
      "\n",
      "Training data size 67227\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 8 times, training data size 38565\n",
      "\n",
      "Training data size 38565\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8314950d63a4f93b6f5d4e8f7807719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3af94b4bac34f36b8ce5727a7cbe9b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1205 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.81\n",
      "Score 0.89\n",
      "Baseline 0.79\n",
      "Ex6.10\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 9 times, training data size 67219\n",
      "\n",
      "Training data size 67219\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 9 times, training data size 38549\n",
      "\n",
      "Training data size 38549\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4b0e3757ee47f3b70198114bb9399b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b164698a762641a492e2a3dcd940daa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.84\n",
      "Score 0.89\n",
      "Baseline 0.79\n",
      "Ex6.11\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 10 times, training data size 67210\n",
      "\n",
      "Training data size 67210\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 10 times, training data size 38531\n",
      "\n",
      "Training data size 38531\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482ef8da930644eb91c7b4e442d60345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edae57a7b134562946d3b7c7ff45229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.87\n",
      "Score 0.86\n",
      "Baseline 0.79\n"
     ]
    }
   ],
   "source": [
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"]\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"UNFREEZE_LAYERS\"] = 2\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "params[\"INITIALIZED_MODEL\"] = \"all-MiniLM-L12-v2\"\n",
    "\n",
    "# print(\"Ex6.1\")\n",
    "# params[\"OCCURENCE_CUTOFF\"] = 0\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.86\n",
    "# # Baseline 0.79\n",
    "\n",
    "# print(\"Ex6.2\")\n",
    "# params[\"OCCURENCE_CUTOFF\"] = 1\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.85\n",
    "# # Baseline 0.79\n",
    "\n",
    "# print(\"Ex6.3\")\n",
    "# params[\"OCCURENCE_CUTOFF\"] = 2\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.83\n",
    "# # Baseline 0.79\n",
    "\n",
    "# print(\"Ex6.4\")\n",
    "# params[\"OCCURENCE_CUTOFF\"] = 3\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.87\n",
    "# # Baseline 0.79\n",
    "\n",
    "# print(\"Ex6.5\")\n",
    "# params[\"OCCURENCE_CUTOFF\"] = 4\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.86\n",
    "# # Baseline 0.79\n",
    "\n",
    "# print(\"Ex6.6\")\n",
    "# params[\"OCCURENCE_CUTOFF\"] = 5\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.9\n",
    "# # Baseline 0.79\n",
    "\n",
    "# print(\"Ex6.7\")\n",
    "# params[\"OCCURENCE_CUTOFF\"] = 6\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.89\n",
    "# # Baseline 0.79\n",
    "\n",
    "# print(\"Ex6.8\")\n",
    "# params[\"OCCURENCE_CUTOFF\"] = 7\n",
    "# dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "# run_pipeline()\n",
    "# # Score 0.88\n",
    "# # Baseline 0.79\n",
    "\n",
    "print(\"Ex6.9\")\n",
    "params[\"OCCURENCE_CUTOFF\"] = 8\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.89\n",
    "# Baseline 0.79\n",
    "\n",
    "print(\"Ex6.10\")\n",
    "params[\"OCCURENCE_CUTOFF\"] = 9\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.89\n",
    "# Baseline 0.79\n",
    "\n",
    "print(\"Ex6.11\")\n",
    "params[\"OCCURENCE_CUTOFF\"] = 10\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline()\n",
    "# Score 0.86\n",
    "# Baseline 0.79\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18176b9c",
   "metadata": {},
   "source": [
    "# Experiment 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2ba851b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex7.10\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, starting with all-mpnet-base-v2, only including words that occur at least 2 times, training data size 67329\n",
      "\n",
      "Training data size 67329\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 2 last layers, excluding Entity and Other labels, starting with all-mpnet-base-v2, only including words that occur at least 2 times, training data size 38651\n",
      "\n",
      "Training data size 38651\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "118cc1f3c4e74b9bb8907635b0b237a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e7557435714a21baab1076a3bead81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.91\n",
      "Score 0.91\n",
      "Baseline 0.79\n"
     ]
    }
   ],
   "source": [
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"]\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"UNFREEZE_LAYERS\"] = 2\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "params[\"OCCURENCE_CUTOFF\"] = 2\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "\n",
    "# print(\"Ex7.1\")\n",
    "# params[\"INITIALIZED_MODEL\"] = \"all-MiniLM-L12-v2\"\n",
    "# run_pipeline()\n",
    "# Score 0.87\n",
    "# Baseline 0.79\n",
    "\n",
    "# print(\"Ex7.2\")\n",
    "# params[\"INITIALIZED_MODEL\"] = \"brjezierski/S3BERT\"\n",
    "# run_pipeline()\n",
    "# # Score 0.86\n",
    "# # Baseline 0.79\n",
    "\n",
    "# print(\"Ex7.3\")\n",
    "# params[\"INITIALIZED_MODEL\"] = \"intfloat/e5-small-v2\"\n",
    "# run_pipeline()\n",
    "# # Score 0.8\n",
    "# # Baseline 0.76\n",
    "\n",
    "# print(\"Ex7.4\")\n",
    "# params[\"INITIALIZED_MODEL\"] = \"thenlper/gte-base\"\n",
    "# run_pipeline()\n",
    "# # Score 0.9\n",
    "# # Baseline 0.8\n",
    "\n",
    "# print(\"Ex7.5\")\n",
    "# params[\"INITIALIZED_MODEL\"] = \"BAAI/bge-base-en-v1.5\"\n",
    "# run_pipeline()\n",
    "# # Score 0.9\n",
    "# # Baseline 0.78\n",
    "\n",
    "# print(\"Ex7.6\")\n",
    "# params[\"INITIALIZED_MODEL\"] = \"intfloat/e5-base-v2\"\n",
    "# run_pipeline()\n",
    "# # Score 0.89\n",
    "# # Baseline 0.81\n",
    "\n",
    "# print(\"Ex7.7\")\n",
    "# params[\"INITIALIZED_MODEL\"] = \"BAAI/bge-small-en-v1.5\"\n",
    "# run_pipeline()\n",
    "# Score 0.84\n",
    "# Baseline 0.77\n",
    "\n",
    "# print(\"Ex7.8\")\n",
    "# params[\"INITIALIZED_MODEL\"] = \"intfloat/e5-small-v2\"\n",
    "# run_pipeline()\n",
    "# # Score 0.82\n",
    "# # Baseline 0.76\n",
    "\n",
    "# print(\"Ex7.9\")\n",
    "# params[\"INITIALIZED_MODEL\"] = \"thenlper/gte-small\"\n",
    "# run_pipeline()\n",
    "# # Score 0.81\n",
    "# # Baseline 0.78\n",
    "\n",
    "print(\"Ex7.10\")\n",
    "params[\"INITIALIZED_MODEL\"] = \"all-mpnet-base-v2\"\n",
    "run_pipeline()\n",
    "# Score 0.91\n",
    "# Baseline 0.79\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0945fa17",
   "metadata": {},
   "source": [
    "# Experiment 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "81b98f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex8.1\n",
      "car_news_df 38818\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 5 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 3 times, training data size67319\n",
      "\n",
      "Training data size 67319\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 5 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 3 times, training data size38635\n",
      "\n",
      "Training data size 38635\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "525c11f9c59b4b97a10a2b51493116dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbaefead49534f4d9f7f745cf938ec58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.8\n",
      "Score 0.9\n",
      "Baseline 0.79\n"
     ]
    }
   ],
   "source": [
    "# best configuration - this or S3BERT?\n",
    "\n",
    "print(\"Ex8.1\")\n",
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"]\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"UNFREEZE_LAYERS\"] = 5\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "params[\"OCCURENCE_CUTOFF\"] = 3\n",
    "params[\"INITIALIZED_MODEL\"] = \"all-MiniLM-L12-v2\"\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline(save_model=True)\n",
    "\n",
    "#S3BERT 0.89\n",
    "#SBERT 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b6bf2",
   "metadata": {},
   "source": [
    "# Experiment 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a364eb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex9.1\n",
      "car_news_df 38818\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 5 last layers, excluding Entity and Other labels, starting with brjezierski/sentence-embeddings-similarity, only including words that occur at least 3 times, training data size67319\n",
      "\n",
      "Training data size 67319\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 5 last layers, excluding Entity and Other labels, starting with brjezierski/sentence-embeddings-similarity, only including words that occur at least 3 times, training data size38635\n",
      "\n",
      "Training data size 38635\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204c43eaf5264b91892c966f578fedbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8733558f1f34aef9104f8fb58811406",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.83\n",
      "Score 0.89\n",
      "Baseline 0.79\n"
     ]
    }
   ],
   "source": [
    "# combined consulting sim->ai+car class\n",
    "\n",
    "print(\"Ex9.1\")\n",
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"]\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"UNFREEZE_LAYERS\"] = 5\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = TrueBAAI/bge-small-en-v1.5\n",
    "params[\"OCCURENCE_CUTOFF\"] = 3\n",
    "params[\"INITIALIZED_MODEL\"] = \"brjezierski/sentence-embeddings-similarity\"\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline(save_model=True)\n",
    "\n",
    "# Score 0.89\n",
    "# Baseline 0.79\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb22d6",
   "metadata": {},
   "source": [
    "# Experiment 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6819095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex10.1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINITIALIZED_MODEL\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrjezierski/sentence-embeddings-similarity-ai-car\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m dataset_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../classification-training-data/snippet_seperate_classifications/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Score 0.93\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Baseline 0.8\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[0;34m(save_model)\u001b[0m\n\u001b[1;32m     10\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWARMUP_STEPS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(big_consulting_df) \u001b[38;5;241m*\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCHS\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDATASETS\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 13\u001b[0m     ai_news_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_news_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mai_news\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     ai_news_df \u001b[38;5;241m=\u001b[39m collect_classification_labels(ai_news_df)\n\u001b[1;32m     15\u001b[0m     ai_news_df \u001b[38;5;241m=\u001b[39m get_top_values(ai_news_df, params)\n",
      "File \u001b[0;32m~/Desktop/master-thesis/notebooks/classification_training_utils.py:74\u001b[0m, in \u001b[0;36mget_news_df\u001b[0;34m(params, dataset_name)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(original_data_path):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(original_data_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 74\u001b[0m         original_dict \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m news_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msnippet\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(original_dict)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# combined ai+car sim->ai+car class\n",
    "\n",
    "print(\"Ex10.1\")\n",
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"]\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"UNFREEZE_LAYERS\"] = 5\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "params[\"OCCURENCE_CUTOFF\"] = 3\n",
    "params[\"INITIALIZED_MODEL\"] = \"brjezierski/sentence-embeddings-similarity-ai-car\"\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "run_pipeline(save_model=True)\n",
    "\n",
    "# Score 0.93\n",
    "# Baseline 0.8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7349be",
   "metadata": {},
   "source": [
    "# Experiment 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "504b16a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex11.1\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 5 last layers, excluding Entity and Other labels, starting with brjezierski/sentence-embeddings-similarity-ai-car, only including words that occur at least 3 times, training data size 262090\n",
      "\n",
      "Training data size 262090\n",
      "Validation data size 94\n",
      "Test data size 90\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 5 last layers, excluding Entity and Other labels, starting with brjezierski/sentence-embeddings-similarity-ai-car, only including words that occur at least 3 times, training data size 177369\n",
      "\n",
      "Training data size 177369\n",
      "Validation data size 94\n",
      "Test data size 90\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b5f9c5776b2445796eb2f06e6d11433",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "816eb9ce7140457898a9ada6125fec50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/5542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# - add labeled data - from best ai+car\n",
    "\n",
    "print(\"Ex11.1\")\n",
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"]\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"UNFREEZE_LAYERS\"] = 5\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "params[\"OCCURENCE_CUTOFF\"] = 3\n",
    "params[\"INITIALIZED_MODEL\"] = \"all-MiniLM-L12-v2\"\n",
    "params[\"NEW_CLASSIFICATIONS\"] = {\n",
    "    \"ai\": \"../classification-training-data/BERTopic_new_classifications/ai.pkl\",\n",
    "    \"car\": \"../classification-training-data/BERTopic_new_classifications/car.pkl\",\n",
    "    \"joint\": \"../classification-training-data/BERTopic_new_classifications/ai_car.pkl\",\n",
    "}\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications_new_classifications/\"\n",
    "run_pipeline(save_model=True)\n",
    "\n",
    "# Score 0.9111111111111111\n",
    "# Baseline 0.8333333333333334"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626ec48f",
   "metadata": {},
   "source": [
    "# Experiment 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7730aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex12.1\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Creating new dev and test triplets\n",
      "e=1, embeddings from snippet, creating a new train-dev-test split, unfreezing 5 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 3 times, training data size 262073\n",
      "\n",
      "Training data size 262073\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Creating new dev and test triplets\n",
      "e=1, embeddings from snippet, creating a new train-dev-test split, unfreezing 5 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 3 times, training data size 177352\n",
      "\n",
      "Training data size 177352\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c4dd8e9bec417a95773f4100efe7a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9a1e8024b148fcbe07deeea1ef2cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/5542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.84\n",
      "Score 0.8\n",
      "Baseline 0.74\n"
     ]
    }
   ],
   "source": [
    "# - add labeled data - from best ai+car - new split\n",
    "\n",
    "print(\"Ex12.1\")\n",
    "params[\"CREATE_NEW_SPLIT\"] = True\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"]\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"UNFREEZE_LAYERS\"] = 5\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "params[\"OCCURENCE_CUTOFF\"] = 3\n",
    "params[\"INITIALIZED_MODEL\"] = \"all-MiniLM-L12-v2\"\n",
    "params[\"NEW_CLASSIFICATIONS\"] = {\n",
    "    \"ai\": \"../classification-training-data/BERTopic_new_classifications/ai.pkl\",\n",
    "    \"car\": \"../classification-training-data/BERTopic_new_classifications/car.pkl\",\n",
    "    \"joint\": \"../classification-training-data/BERTopic_new_classifications/ai_car.pkl\",\n",
    "}\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications_new_classifications_create_new_split/\"\n",
    "run_pipeline(save_model=True)\n",
    "\n",
    "# Score 0.8\n",
    "# Baseline 0.74"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485cce9",
   "metadata": {},
   "source": [
    "# Experiment 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e718bff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex11.1\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 5 last layers, excluding Entity and Other labels, starting with brjezierski/sentence-embeddings-similarity-ai-car, only including words that occur at least 3 times, training data size 262090\n",
      "\n",
      "Training data size 262090\n",
      "Validation data size 94\n",
      "Test data size 90\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 5 last layers, excluding Entity and Other labels, starting with brjezierski/sentence-embeddings-similarity-ai-car, only including words that occur at least 3 times, training data size 177369\n",
      "\n",
      "Training data size 177369\n",
      "Validation data size 94\n",
      "Test data size 90\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7b7b0c984124dbe83d4a1ff895dfdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f9582aef5514935b521d3205bab9aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/5542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.8297872340425532\n",
      "Score 0.8777777777777778\n",
      "Baseline 0.8444444444444444\n"
     ]
    }
   ],
   "source": [
    "# - best classification on ai+car (data augmentation) and - combined ai+car sim->class\n",
    "\n",
    "print(\"Ex11.1\")\n",
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"]\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"UNFREEZE_LAYERS\"] = 5\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "params[\"OCCURENCE_CUTOFF\"] = 3\n",
    "params[\"INITIALIZED_MODEL\"] = \"brjezierski/sentence-embeddings-similarity-ai-car\"\n",
    "params[\"NEW_CLASSIFICATIONS\"] = {\n",
    "    \"ai\": \"../classification-training-data/BERTopic_new_classifications/ai.pkl\",\n",
    "    \"car\": \"../classification-training-data/BERTopic_new_classifications/car.pkl\",\n",
    "    \"joint\": \"../classification-training-data/BERTopic_new_classifications/ai_car.pkl\",\n",
    "}\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications_new_classifications/\"\n",
    "run_pipeline(save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e5e5a",
   "metadata": {},
   "source": [
    "# Experiment 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd46b2b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ex14.1\n",
      "Using datasets: ['ai', 'car']\n",
      "Training with multiple objectives\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 5 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 3 times, training data size 67319\n",
      "\n",
      "Training data size 67319\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Reading existing dev and test triplets\n",
      "e=1, embeddings from snippet, unfreezing 5 last layers, excluding Entity and Other labels, starting with all-MiniLM-L12-v2, only including words that occur at least 3 times, training data size 38635\n",
      "\n",
      "Training data size 38635\n",
      "Validation data size 100\n",
      "Test data size 100\n",
      "Performance before fine-tuning:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4895b9ec59c841ff9e8a277b2610e73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd2bda1882b4365b3d303ebe894612a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 0/1207 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score at epoch 0, step -1: 0.83\n",
      "Score 0.89\n",
      "Baseline 0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Ex14.1\")\n",
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"EPOCHS\"] = 1\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"]\n",
    "params[\"SNIPPET_COLUMN_NAME\"] = 'snippet'\n",
    "params[\"UNFREEZE_LAYERS\"] = 5\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "params[\"OCCURENCE_CUTOFF\"] = 3\n",
    "params[\"INITIALIZED_MODEL\"] = \"all-MiniLM-L12-v2\"\n",
    "params[\"LOSS\"] = \"BatchHardTripletLoss\"\n",
    "dataset_dir = \"../classification-training-data/snippet_seperate_classifications/\"\n",
    "\n",
    "run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2dbfd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
