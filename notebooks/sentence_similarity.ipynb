{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df14d81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import classification_training_utils\n",
    "importlib.reload(classification_training_utils)\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "import sys\n",
    "import dimensionality_reduction\n",
    "import json \n",
    "import numpy as np\n",
    "from numpy import dot\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly.offline as pyo\n",
    "import plotly.express as px\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "from ast import literal_eval\n",
    "\n",
    "from utils import aggregate_embeddings, cosine_similarity, print_similarity_samples, load_model, country_code_map, split_into_list, replace_nan_with, collect_column_values, create_replace_no_tags_embeddings\n",
    "from classification_training_utils import get_big_consulting_df, get_news_df, get_top_values, get_relevant_classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1562ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params['USE_REPLACE_DATA'] = True\n",
    "params['USE_ORIGINAL_DATA'] = False\n",
    "params['ALLOW_PAIRS_OF_SAME_COMPANIES'] = True\n",
    "params['PAIR_RANDOM_ROWS'] = False\n",
    "params['READ_SAVED_DATASET'] = False\n",
    "params['CREATE_NEW_SPLIT'] = False\n",
    "params[\"DATASETS\"] = [\"ai\", \"car\"] # [\"ai\", \"car\", \"consulting\", \"consulting2\"]\n",
    "params[\"SIMILARITY\"] = [\"company\"]\n",
    "params[\"OUTPUT_DIR\"] = \"../similarity-training-data/consulting\"\n",
    "params[\"COMPANY_EMBEDDINGS\"] = 'company_embedding_dicts_sbert.pickle'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e552f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_rows(df, params):\n",
    "    df1 = df.iloc[::2].reset_index(drop=True)  # odd-indexed rows\n",
    "    df2 = df.iloc[1::2].reset_index(drop=True)  # even-indexed rows\n",
    "    df1.columns = [f\"{col}1\" for col in df1.columns]\n",
    "    df2.columns = [f\"{col}2\" for col in df2.columns]\n",
    "    merged_df = pd.DataFrame(columns=df1.columns.tolist() + df2.columns.tolist())\n",
    "\n",
    "    def get_random_row(df, random_state):\n",
    "        random.seed(random_state)\n",
    "        sampled_row_index = random.choice(df2.index)\n",
    "        sampled_row_df2 = df2.loc[sampled_row_index]\n",
    "        return sampled_row_index, sampled_row_df2\n",
    "\n",
    "    if not params['PAIR_RANDOM_ROWS']:\n",
    "        merged_df = pd.concat([df1, df2], axis=1).reset_index(drop=True)\n",
    "    else:\n",
    "        for i in tqdm(range(0, len(df2))):\n",
    "            row_df1 = df1.loc[i]\n",
    "            random_state = 42\n",
    "            row_df2_index, row_df2 = get_random_row(df2, random_state)\n",
    "\n",
    "            count = 0\n",
    "            if not params['ALLOW_PAIRS_OF_SAME_COMPANIES']:\n",
    "                while row_df2['company2'] == row_df1['company1']:\n",
    "                    random_state -= 1\n",
    "                    row_df2_index, row_df2 = get_random_row(df2, random_state)\n",
    "        #             print(sampled_row_df2['company2'])\n",
    "                    count += 1\n",
    "                    if count == 50:\n",
    "                        break\n",
    "            merged_row = pd.concat([row_df1, row_df2], axis=0)\n",
    "            merged_df.loc[len(merged_df.index)] = merged_row\n",
    "            df2 = df2.drop(row_df2_index)\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc0ce483",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_acronym_map = {'ai': 'artificial intelligence', 'llm':'large language models', 'nlp': 'natural language processing'}\n",
    "\n",
    "def preprocess_df(df, params):\n",
    "    df = df.drop_duplicates(subset=['replace'])\n",
    "    \n",
    "    def replace_keywords_with_acronyms(row, keyword_acronym_map):\n",
    "        keywords = row['keywords']\n",
    "        updated_keywords = set()\n",
    "        for kw in keywords:\n",
    "            updated_keywords.add(keyword_acronym_map[kw] if kw in keyword_acronym_map.keys() else kw)\n",
    "        row['keywords'] = list(updated_keywords)\n",
    "        return row\n",
    "\n",
    "#     df = df.drop(columns=[\"index\", \"tooltip\", \"score\", \"id\", \"company\", \"relationEntity\", \"relationEntityType\", \"country\", \"similarity\", \"Unnamed: 4\", \"Unnamed: 5\"])\n",
    "    cols_with_lists = ['keywords', 'classification']\n",
    "    df = split_into_list(df, cols_with_lists)\n",
    "    df = df.apply(lambda row: replace_keywords_with_acronyms(row, keyword_acronym_map), axis=1)\n",
    "    df['id'] = df.index\n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    df = merge_rows(df, params)\n",
    "    return df\n",
    "\n",
    "def merge_split_and_save_df(df, params, dataset_dir, val_test_size=100):\n",
    "    # Preprocess and merge rows\n",
    "    df = preprocess_df(df, params)\n",
    "    # Split\n",
    "    test_df = df[:val_test_size]\n",
    "    val_df = df[val_test_size:val_test_size*2]\n",
    "    train_df = df[val_test_size*2:]\n",
    "    train_df.dropna(subset=['replace2'], inplace=True)\n",
    "#     # Save - too big to save\n",
    "#     os.makedirs(dataset_dir)\n",
    "#     train_df.to_csv(f'{dataset_dir}train.tsv', sep='\\t')\n",
    "#     val_df.to_csv(f'{dataset_dir}val.tsv', sep='\\t')\n",
    "#     test_df.to_csv(f'{dataset_dir}test.tsv', sep='\\t')\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def read_split_df(dataset_dir):\n",
    "    cols_with_lists = ['keywords1', 'classification1', 'keywords2', 'classification2']\n",
    "    test_df = pd.read_csv(f'{dataset_dir}test.tsv', sep='\\t', index_col=0)\n",
    "    test_df = split_into_list(test_df, cols_with_lists)\n",
    "    val_df = pd.read_csv(f'{dataset_dir}val.tsv', sep='\\t', index_col=0)\n",
    "    val_df = split_into_list(val_df, cols_with_lists)\n",
    "    train_df = pd.read_csv(f'{dataset_dir}train.tsv', sep='\\t', index_col=0)\n",
    "    if 'snippet2.1' in train_df.columns:\n",
    "        train_df = train_df.drop(columns=['snippet2.1'])\n",
    "    train_df = split_into_list(train_df, cols_with_lists)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def get_split_df(params, df=None, dataset_dir='../dataset/'):\n",
    "    train_df, val_df, test_df = read_split_df(dataset_dir) if os.path.exists(dataset_dir) else merge_split_and_save_df(df, params, dataset_dir, val_test_size=100)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def save_dataset(train_df, val_df, test_df, params):\n",
    "    if not os.path.exists(params[\"OUTPUT_DIR\"]):\n",
    "        os.makedirs(params[\"OUTPUT_DIR\"])\n",
    "    # drop embeddings because they take up a lot of space\n",
    "    if 'embedding1' in train_df.columns:\n",
    "        train_df = train_df.drop(columns=['embedding1', 'embedding2'])\n",
    "        val_df = val_df.drop(columns=['embedding1', 'embedding2'])\n",
    "        test_df = test_df.drop(columns=['embedding1', 'embedding2'])\n",
    "    train_df.to_csv(f'{params[\"OUTPUT_DIR\"]}train.tsv', sep='\\t')\n",
    "    val_df.to_csv(f'{params[\"OUTPUT_DIR\"]}val.tsv', sep='\\t')\n",
    "    test_df.to_csv(f'{params[\"OUTPUT_DIR\"]}test.tsv', sep='\\t')\n",
    "\n",
    "def read_dataset():\n",
    "    if 'company' in params[\"SIMILARITY\"] and \"ai\" in params[\"DATASETS\"] and \"car\" in params[\"DATASETS\"]:\n",
    "        read_dir = '../similarity-training-data/ai_car/sbert-company-0.0/'\n",
    "        test_df = pd.read_csv(f'{read_dir}test.tsv', sep='\\t', index_col=0)\n",
    "        val_df = pd.read_csv(f'{read_dir}val.tsv', sep='\\t', index_col=0)\n",
    "        train_df = pd.read_csv(f'{read_dir}train.tsv', sep='\\t', index_col=0)\n",
    "        if not \"company_similarity\" in train_df.columns:\n",
    "            car_dir = '../glanos-data/datasets/car_news_w_companies.tsv'\n",
    "            ai_dir = '../glanos-data/datasets/ai_news_w_companies.tsv'\n",
    "            car_df_w_comp = pd.read_csv(car_dir, sep='\\t')\n",
    "            ai_df_w_comp = pd.read_csv(car_dir, sep='\\t')\n",
    "            df_w_comp = pd.concat([car_df_w_comp, ai_df_w_comp], axis=0).reset_index(drop=True).drop(columns=['replace', 'classification', 'keywords'])\n",
    "\n",
    "            df_w_comp = df_w_comp.drop_duplicates(subset='snippet', keep='first')\n",
    "            company_dict = df_w_comp.set_index('snippet').to_dict()['company']\n",
    "            relationEntity_dict = df_w_comp.set_index('snippet').to_dict()['relationEntity']\n",
    "            relationEntityType_dict = df_w_comp.set_index('snippet').to_dict()['relationEntityType']\n",
    "\n",
    "            def update_df_with_company(df):\n",
    "                for index, row in df.iterrows():\n",
    "                    current_snippet = df.at[index, 'snippet1']\n",
    "                    df.at[index, 'company1'] = company_dict[current_snippet] if current_snippet in company_dict else ''\n",
    "                    df.at[index, 'relationEntity1'] = relationEntity_dict[current_snippet] if current_snippet in relationEntity_dict else ''\n",
    "                    df.at[index, 'relationEntityType1'] = relationEntityType_dict[current_snippet] if current_snippet in relationEntityType_dict else ''\n",
    "                    current_snippet = df.at[index, 'snippet2']\n",
    "                    df.at[index, 'company2'] = company_dict[current_snippet] if current_snippet in company_dict else ''\n",
    "                    df.at[index, 'relationEntity2'] = relationEntity_dict[current_snippet] if current_snippet in relationEntity_dict else ''\n",
    "                    df.at[index, 'relationEntityType2'] = relationEntityType_dict[current_snippet] if current_snippet in relationEntityType_dict else ''\n",
    "                return df\n",
    "\n",
    "            train_df = update_df_with_company(train_df)\n",
    "            print('train_df', len(train_df))\n",
    "            val_df = update_df_with_company(val_df)\n",
    "            print('val_df', len(val_df))\n",
    "            test_df = update_df_with_company(test_df)\n",
    "            print('test_df', len(test_df))\n",
    "            print('Updated with companies')\n",
    "    else:\n",
    "        if \"ai\" in params[\"DATASETS\"] and \"car\" in params[\"DATASETS\"]:\n",
    "            read_dir = '../similarity-training-data/replace/'\n",
    "        else: \n",
    "            read_dir = '../similarity-training-data/consulting/'\n",
    "        test_df = pd.read_csv(f'{read_dir}test.tsv', sep='\\t', index_col=0)\n",
    "        val_df = pd.read_csv(f'{read_dir}val.tsv', sep='\\t', index_col=0)\n",
    "        train_df = pd.read_csv(f'{read_dir}train.tsv', sep='\\t', index_col=0)\n",
    "\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05451933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    try:\n",
    "        a = np.array(list(a), dtype=np.float32)\n",
    "        b = np.array(list(b), dtype=np.float32)\n",
    "        return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "    except ValueError as ve:\n",
    "        print('ValueError')\n",
    "        return 0.0\n",
    "\n",
    "def convert_ndarray(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "    \n",
    "def find_company_name(words, company_list):\n",
    "    for i in range(len(words), 0, -1):\n",
    "        substring = ' '.join(words[:i])\n",
    "        if substring in company_list:\n",
    "            return substring\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_company_names(company, relation_entity, relation_entity_type, company_list):\n",
    "    if not pd.isna(company):\n",
    "        possible_company_names = [company.lower()]\n",
    "    else:\n",
    "        possible_company_names = []\n",
    "    company_names = []\n",
    "    if relation_entity_type == 'company' and not pd.isna(relation_entity):\n",
    "        possible_company_names.append(relation_entity.lower())\n",
    "    for possible_company_name in possible_company_names:\n",
    "        if possible_company_name in company_list:\n",
    "            company_name = possible_company_name\n",
    "            company_names.append(possible_company_name)\n",
    "        else:\n",
    "            possible_company_names = possible_company_name.split(' ')\n",
    "            company_name = find_company_name(possible_company_names, company_list)\n",
    "            if company_name is not None:\n",
    "                company_names.append(company_name)\n",
    "    return company_names\n",
    "\n",
    "\n",
    "def company_similarity(row, company_embedding_dict):\n",
    "    company_list = list(company_embedding_dict.keys())\n",
    "    companies1 = get_company_names(row['company1'], row['relationEntity1'], row['relationEntityType1'], company_list)\n",
    "    companies2 = get_company_names(row['company2'], row['relationEntity2'], row['relationEntityType2'], company_list)\n",
    "    \n",
    "    embeddings1 = aggregate_embeddings(companies1, company_embedding_dict)\n",
    "    embeddings2 = aggregate_embeddings(companies2, company_embedding_dict)\n",
    "        \n",
    "    if companies1 == [] or companies2 == []:\n",
    "        similarity = row['similarity']\n",
    "    else:\n",
    "        similarity = cosine_similarity(embeddings1, embeddings2)\n",
    "    row['company_similarity'], row['companies1'], row['companies2'] = convert_ndarray(similarity), companies1, companies2\n",
    "    \n",
    "    return row\n",
    "\n",
    "def keyword_with_company_similarity(row, keyword_dict, company_embedding_dict):\n",
    "    '''\n",
    "    If the keyword is a company name, use company embeddings instead of SBERT\n",
    "    '''\n",
    "    company_list = list(company_embedding_dict.keys())\n",
    "    temp_dict_1 = {}\n",
    "    temp_dict_2 = {}\n",
    "    \n",
    "    for kw in row['keywords1']: \n",
    "        if kw in company_list:\n",
    "            temp_dict_1[kw] = company_embedding_dict[kw]\n",
    "        elif kw in keyword_dict.keys():\n",
    "            temp_dict_1[kw] = keyword_dict[kw]\n",
    "                \n",
    "    for kw in row['keywords2']: \n",
    "        if kw in company_list:\n",
    "            temp_dict_2[kw] = company_embedding_dict[kw]\n",
    "        elif kw in keyword_dict.keys():\n",
    "            temp_dict_2[kw] = keyword_dict[kw]\n",
    "            \n",
    "    embeddings1 = aggregate_embeddings(temp_dict_1.keys(), temp_dict_1)\n",
    "    embeddings2 = aggregate_embeddings(temp_dict_2.keys(), temp_dict_2)\n",
    "        \n",
    "    if list(temp_dict_1.keys()) == [] or list(temp_dict_2.keys()) == []:\n",
    "        similarity = row['similarity']\n",
    "    else:\n",
    "        similarity = cosine_similarity(embeddings1, embeddings2)\n",
    "    row['keyword_similarity'] = convert_ndarray(similarity)\n",
    "    \n",
    "    return row\n",
    "\n",
    "def convert_ndarray(x):\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return 0.0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def company_similarity_df(df, company_embedding_dict, verbose=False):\n",
    "    df = df.progress_apply(lambda row: company_similarity(row, company_embedding_dict), axis=1)\n",
    "    if verbose:\n",
    "        print_similarity_samples(df, 'company_similarity', sample_size=5)\n",
    "    return df\n",
    "\n",
    "def keyword_similarity_with_companies_df(df, keyword_dict, company_dict, verbose=False):\n",
    "    df = df.progress_apply(lambda row: keyword_with_company_similarity(row, keyword_dict, company_dict), axis=1)\n",
    "    if verbose:\n",
    "        print_similarity_samples(df, 'keyword_similarity', sample_size=5)\n",
    "    return df\n",
    "\n",
    "\n",
    "def column_similarity(row, column_name, embedding_dict):\n",
    "    '''\n",
    "    If keywords or classification are empty, set the simialrity same as similarity of the whole sentence\n",
    "    '''\n",
    "    column1, column2 = row[f'{column_name}1'], row[f'{column_name}2']\n",
    "    if type(column1) != list:\n",
    "        column1 = [column1]\n",
    "        column2 = [column2]\n",
    "        if pd.isna(column1) or pd.isna(column2):\n",
    "            row[f'{column_name}_similarity'] = row['similarity']\n",
    "            return row\n",
    "    else:\n",
    "        if len(column1) == 0 or len(column2) == 0:\n",
    "            row[f'{column_name}_similarity'] = row['similarity']\n",
    "            return row\n",
    "            \n",
    "    to_lower = column_name != 'country'\n",
    "    embeddings1 = aggregate_embeddings(column1, embedding_dict, to_lower=to_lower)\n",
    "    embeddings2 = aggregate_embeddings(column2, embedding_dict, to_lower=to_lower)\n",
    "    row[f'{column_name}_similarity'] = convert_ndarray(cosine_similarity(embeddings1, embeddings2))\n",
    "    \n",
    "    return row\n",
    "\n",
    "def preprocess_column(df, column_name, words_to_remove=[]):\n",
    "    df[f'{column_name}1'] = df[f'{column_name}1'].apply(lambda x: x if type(x) == list else literal_eval(x))\n",
    "    df[f'{column_name}2'] = df[f'{column_name}2'].apply(lambda x: x if type(x) == list else literal_eval(x))\n",
    "    if tokens_to_remove and len(tokens_to_remove) > 0:\n",
    "        for token_to_remove in tokens_to_remove:\n",
    "            df[f'{column_name}1'] = df[f'{column_name}1'].apply(lambda tokens: [token for token in tokens if token != token_to_remove] if token_to_remove in tokens else tokens)\n",
    "            df[f'{column_name}2'] = df[f'{column_name}2'].apply(lambda tokens: [token for token in tokens if token != token_to_remove] if token_to_remove in tokens else tokens)\n",
    "    return df\n",
    "\n",
    "def preprocess_classification(df):\n",
    "    df['classification1'] = df['classification1'].apply(lambda x: [] if 'entity' in x or 'other' in x else x)\n",
    "    df['classification2'] = df['classification2'].apply(lambda x: [] if 'entity' in x or 'other' in x else x)\n",
    "    df['classification1'] = df['classification1'].apply(lambda x: x if type(x) == list else literal_eval(x))\n",
    "    df['classification2'] = df['classification2'].apply(lambda x: x if type(x) == list else literal_eval(x))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c943ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# all_keywords = ai_news_df['keywords'].str.split('|').explode().tolist()\n",
    "# word_count = Counter(all_keywords)\n",
    "\n",
    "# # Print the word frequencies\n",
    "# for word, count in word_count.items():\n",
    "#     print(f'{word}: {count}')\n",
    "\n",
    "\n",
    "def run_pipeline():\n",
    "    if params['CREATE_NEW_SPLIT']:\n",
    "        train_datasets, val_datasets, test_datasets = [], [], []\n",
    "        if 'consulting' in params['DATASETS']:\n",
    "            big_consulting_dataset_dir = '../dataset/big_consulting/'\n",
    "            big_consulting_df = get_big_consulting_df(params)\n",
    "            big_consulting_train_df, big_consulting_val_df, big_consulting_test_df = get_split_df(params, df=big_consulting_df, dataset_dir=big_consulting_dataset_dir)\n",
    "            train_datasets.append(big_consulting_train_df)\n",
    "            val_datasets.append(big_consulting_val_df)\n",
    "            test_datasets.append(big_consulting_test_df)\n",
    "\n",
    "        if 'ai' in params['DATASETS']:\n",
    "            ai_news_dataset_dir = '../dataset/ai_news/'\n",
    "            ai_news_df = get_news_df(params, 'ai_news')\n",
    "            ai_news_train_df, ai_news_val_df, ai_news_test_df = get_split_df(params, df=ai_news_df, dataset_dir=ai_news_dataset_dir)\n",
    "            train_datasets.append(ai_news_train_df)\n",
    "            val_datasets.append(ai_news_val_df)\n",
    "            test_datasets.append(ai_news_test_df)\n",
    "\n",
    "        if 'car' in params['DATASETS']:\n",
    "            car_news_dataset_dir = '../dataset/car_news/'\n",
    "            car_news_df = get_news_df(params, 'car_news')\n",
    "            car_news_train_df, car_news_val_df, car_news_test_df = get_split_df(params, df=car_news_df, dataset_dir=car_news_dataset_dir)\n",
    "            train_datasets.append(car_news_train_df)\n",
    "            val_datasets.append(car_news_val_df)\n",
    "            test_datasets.append(car_news_test_df)\n",
    "\n",
    "        train_df = pd.concat(train_datasets, axis=0).reset_index(drop=True)\n",
    "        val_df = pd.concat(val_datasets, axis=0).reset_index(drop=True)\n",
    "        test_df = pd.concat(test_datasets, axis=0).reset_index(drop=True)\n",
    "\n",
    "        train_df['similarity'] = train_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "        val_df['similarity'] = val_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "        test_df['similarity'] = test_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "\n",
    "\n",
    "        params[\"COLUMNS\"] = train_df.columns\n",
    "\n",
    "        prefix = \"../glanos-data/embeddings/\"\n",
    "\n",
    "        with open(f'{prefix}ai_news_replace_no_tags.pickle', 'rb') as f:\n",
    "            replace_no_tags_embeddings = pickle.load(f)\n",
    "            with open(f'{prefix}car_news_replace_no_tags.pickle', 'rb') as f:\n",
    "                replace_no_tags_embeddings.update(pickle.load(f))\n",
    "\n",
    "        test_df = create_replace_no_tags_embeddings(test_df, replace_no_tags_embeddings)\n",
    "        val_df = create_replace_no_tags_embeddings(val_df, replace_no_tags_embeddings)\n",
    "        train_df = create_replace_no_tags_embeddings(train_df, replace_no_tags_embeddings)\n",
    "\n",
    "\n",
    "        test_df['similarity'] = test_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "        val_df['similarity'] = test_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "        train_df['similarity'] = test_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "\n",
    "\n",
    "    else:\n",
    "        train_df, val_df, test_df = read_dataset()\n",
    "        \n",
    "    prefix = '../glanos-data/embeddings/'\n",
    "    with open(f'{prefix}ai_news_snippet.pickle', 'rb') as f:\n",
    "        ai_news_dict = pickle.load(f)\n",
    "    with open(f'{prefix}car_news_snippet.pickle', 'rb') as f:\n",
    "        car_news_dict = pickle.load(f)\n",
    "    embeddings_dict = ai_news_dict\n",
    "    embeddings_dict.update(car_news_dict)\n",
    "\n",
    "    train_df[\"embedding1\"] = train_df[\"snippet1\"].apply(lambda x: embeddings_dict.get(x))\n",
    "    train_df[\"embedding2\"] = train_df[\"snippet2\"].apply(lambda x: embeddings_dict.get(x))\n",
    "    val_df[\"embedding1\"] = val_df[\"snippet1\"].apply(lambda x: embeddings_dict.get(x))\n",
    "    val_df[\"embedding2\"] = val_df[\"snippet2\"].apply(lambda x: embeddings_dict.get(x))\n",
    "    test_df[\"embedding1\"] = test_df[\"snippet1\"].apply(lambda x: embeddings_dict.get(x))\n",
    "    test_df[\"embedding2\"] = test_df[\"snippet2\"].apply(lambda x: embeddings_dict.get(x))\n",
    "\n",
    "    train_df['similarity'] = train_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "    val_df['similarity'] = val_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "    test_df['similarity'] = test_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "\n",
    "#     if 'keywords' in params[\"SIMILARITY\"]:\n",
    "#         with open(f'{prefix}{params[\"COMPANY_EMBEDDINGS\"]}.pickle', 'rb') as f:\n",
    "#             company_embedding_dict = pickle.load(f)\n",
    "#         print('company_embedding_dict', len(company_embedding_dict))\n",
    "#         with open(f'{prefix}kw_embedding_dict.pickle', 'rb') as f:\n",
    "#             kw_embeddings = pickle.load(f)\n",
    "#         test_df = preprocess_column(test_df, 'keywords', words_to_remove=['artificial intelligence'])\n",
    "#         val_df = preprocess_column(val_df, 'keywords', words_to_remove=['artificial intelligence'])\n",
    "#         train_df = preprocess_column(train_df, 'keywords', words_to_remove=['artificial intelligence'])\n",
    "#         test_df = keyword_similarity_with_companies_df(test_df, kw_embeddings, company_embedding_dict)\n",
    "#         val_df = keyword_similarity_with_companies_df(val_df, kw_embeddings, company_embedding_dict)\n",
    "#         train_df = keyword_similarity_with_companies_df(train_df, kw_embeddings, company_embedding_dict)\n",
    "\n",
    "#     if 'classification' in params[\"SIMILARITY\"]:\n",
    "#         if \"ai\" in params[\"DATASETS\"] and \"car\" in params[\"DATASETS\"]:\n",
    "#             with open(f'{prefix}car_news_classification.pickle', 'rb') as f:\n",
    "#                 classsification_embeddings = pickle.load(f)\n",
    "#                 with open(f'{prefix}ai_news_classification.pickle', 'rb') as f:\n",
    "#                     classsification_embeddings.update(pickle.load(f))\n",
    "#         else:\n",
    "#             with open(f'{prefix}class_embedding_dict.pickle', 'rb') as f:\n",
    "#                 classsification_embeddings = pickle.load(f)\n",
    "\n",
    "#         test_df = preprocess_column(test_df, 'classification', words_to_remove=['entity', 'other'])\n",
    "#         val_df = preprocess_column(val_df, 'classification', words_to_remove=['entity', 'other'])\n",
    "#         train_df = preprocess_column(train_df, 'classification', words_to_remove=['entity', 'other'])\n",
    "#         test_df = test_df.progress_apply(lambda row: column_similarity(row, 'classification', classsification_embeddings), axis=1)\n",
    "#         val_df = val_df.progress_apply(lambda row: column_similarity(row, 'classification', classsification_embeddings), axis=1)\n",
    "#         train_df = train_df.progress_apply(lambda row: column_similarity(row, 'classification', classsification_embeddings), axis=1)\n",
    "\n",
    "#     if 'company' in params[\"SIMILARITY\"] and not \"company_similarity\" in train_df.columns:\n",
    "#         with open(f'{prefix}{params[\"COMPANY_EMBEDDINGS\"]}.pickle', 'rb') as f:\n",
    "#             company_embedding_dict = pickle.load(f)\n",
    "\n",
    "#         test_df = company_similarity_df(test_df, company_embedding_dict)\n",
    "#         val_df = company_similarity_df(val_df, company_embedding_dict)\n",
    "#         train_df = company_similarity_df(train_df, company_embedding_dict)\n",
    "\n",
    "#     if 'country' in params[\"SIMILARITY\"]:\n",
    "#         country_embeddings = country_code_map()\n",
    "#         test_df = test_df.progress_apply(lambda row: column_similarity(row, 'country', country_embeddings), axis=1)\n",
    "#         val_df = val_df.progress_apply(lambda row: column_similarity(row, 'country', country_embeddings), axis=1)\n",
    "#         train_df = train_df.progress_apply(lambda row: column_similarity(row, 'country', country_embeddings), axis=1)\n",
    "\n",
    "    save_dataset(train_df, val_df, test_df, params)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb69c0c7",
   "metadata": {},
   "source": [
    "# Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec947e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params[\"DATASETS\"] = [\"consulting\"]\n",
    "# params[\"SIMILARITY\"] = [\"company\"]\n",
    "# for definition_weight in np.arange(0.0, 1.1, 0.1):\n",
    "#     suffix = str(definition_weight)[:3]\n",
    "#     params[\"OUTPUT_DIR\"] = f\"../similarity-training-data/consulting/glanos-company-{suffix}/\"\n",
    "#     params[\"COMPANY_EMBEDDINGS\"] = f\"company_embedding_dicts_glanos_{suffix}\"\n",
    "#     run_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fd4d63",
   "metadata": {},
   "source": [
    "# Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "697d9ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rq/14wnz0xs6xs5z8gzsf2j1g940000gn/T/ipykernel_22729/2023217641.py:11: DtypeWarning:\n",
      "\n",
      "Columns (14,15,16,17,18,19) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "\n",
      "100%|█████████████████████████████████████████████| 200/200 [00:00<00:00, 20883.81it/s]\n",
      "100%|█████████████████████████████████████████████| 200/200 [00:00<00:00, 21092.27it/s]\n",
      "100%|███████████████████████████████████████| 219815/219815 [00:08<00:00, 25787.20it/s]\n"
     ]
    }
   ],
   "source": [
    "params[\"SIMILARITY\"] = [\"company\", \"keywords\", \"classification\"]\n",
    "tokens_to_remove = None\n",
    "params[\"INPUT_DIR\"] = f\"../similarity-training-data/ai_car/replace_no_tags/sbert-company-0.0/\"\n",
    "params[\"OUTPUT_DIR\"] = f\"../similarity-training-data/ai_car/snippet/sbert-company-0.0/\"\n",
    "params[\"COMPANY_EMBEDDINGS\"] = f\"company_embedding_dicts_sbert_0.0\"\n",
    "# run_pipeline()\n",
    "\n",
    "# Update similarity\n",
    "test_df = pd.read_csv(f'{params[\"INPUT_DIR\"]}test.tsv', sep='\\t', index_col=0)\n",
    "val_df = pd.read_csv(f'{params[\"INPUT_DIR\"]}val.tsv', sep='\\t', index_col=0)\n",
    "train_df = pd.read_csv(f'{params[\"INPUT_DIR\"]}train.tsv', sep='\\t', index_col=0)\n",
    "        \n",
    "prefix = '../glanos-data/embeddings/'\n",
    "with open(f'{prefix}ai_news_snippet.pickle', 'rb') as f:\n",
    "    ai_news_dict = pickle.load(f)\n",
    "with open(f'{prefix}car_news_snippet.pickle', 'rb') as f:\n",
    "    car_news_dict = pickle.load(f)\n",
    "embeddings_dict = ai_news_dict\n",
    "embeddings_dict.update(car_news_dict)\n",
    "\n",
    "test_df[\"embedding1\"] = test_df[\"snippet1\"].apply(lambda x: embeddings_dict.get(x))\n",
    "test_df[\"embedding2\"] = test_df[\"snippet2\"].apply(lambda x: embeddings_dict.get(x))\n",
    "test_df['similarity'] = test_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "test_df = test_df.drop(columns=['embedding1', 'embedding2'])\n",
    "test_df.to_csv(f'{params[\"OUTPUT_DIR\"]}test.tsv', sep='\\t')\n",
    "\n",
    "val_df[\"embedding1\"] = val_df[\"snippet1\"].apply(lambda x: embeddings_dict.get(x))\n",
    "val_df[\"embedding2\"] = val_df[\"snippet2\"].apply(lambda x: embeddings_dict.get(x))\n",
    "val_df['similarity'] = val_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "val_df = val_df.drop(columns=['embedding1', 'embedding2'])\n",
    "val_df.to_csv(f'{params[\"OUTPUT_DIR\"]}val.tsv', sep='\\t')\n",
    "\n",
    "train_df[\"embedding1\"] = train_df[\"snippet1\"].apply(lambda x: embeddings_dict.get(x))\n",
    "train_df[\"embedding2\"] = train_df[\"snippet2\"].apply(lambda x: embeddings_dict.get(x))\n",
    "train_df['similarity'] = train_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "train_df = train_df.drop(columns=['embedding1', 'embedding2'])\n",
    "train_df.to_csv(f'{params[\"OUTPUT_DIR\"]}train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e917589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def split_dataframe(df, value):\n",
    "    df_1 = df.loc[:, df.columns.str.contains(value)]\n",
    "    df_2 = df.loc[:, ~df.columns.str.contains(value)]\n",
    "    return df_1, df_2\n",
    "\n",
    "def remove_trailing_number(column):\n",
    "    if column[-1] in ['1', '2']:\n",
    "        return column[:-1]\n",
    "    return column\n",
    "\n",
    "df_1, df_2 = split_dataframe(train_df, '1')\n",
    "# Remove trailing number from column names\n",
    "df_1.columns = df_1.columns.map(remove_trailing_number)\n",
    "df_2.columns = df_2.columns.map(remove_trailing_number)\n",
    "\n",
    "# Append the two DataFrames together\n",
    "combined_df = pd.concat([df_1, df_2], axis=0).reset_index()\n",
    "\n",
    "def has_verb(sentence, to_print=False):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    tags = pos_tag(tokens)\n",
    "    \n",
    "    for token, tag in zip(tokens,tags):\n",
    "        if to_print:\n",
    "            print(token, ':', tag)\n",
    "        if tag[1].startswith('VB'):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def has_no_verb(sentence):\n",
    "    return not has_verb(sentence)\n",
    "\n",
    "def has_no_verb_2(sentence):\n",
    "    return not has_verb(sentence, to_print=True)\n",
    "\n",
    "\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# # short_snippets = combined_df[(combined_df['snippet'].apply(str).str.len() < 40)][['snippet']] #  | (combined_df['snippet'].apply(str.lower).apply(has_no_verb))\n",
    "# # print(short_snippets)\n",
    "# combined_df = combined_df[(combined_df['snippet'].apply(str).str.len() >= 40)] #  | (combined_df['snippet'].apply(str).apply(has_verb))\n",
    "# len(combined_df)\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82c5d3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "if params['READ_SAVED_DATASET']:\n",
    "    train_df = merge_rows(combined_df, params)\n",
    "\n",
    "\n",
    "    train_df['embedding2'] = train_df['embedding2'].apply(lambda x: \n",
    "                               np.fromstring(\n",
    "                                   x.replace('\\n','')\n",
    "                                    .replace('[','')\n",
    "                                    .replace(']','')\n",
    "                                    .replace('  ',' '), sep=' '))\n",
    "    train_df['embedding1'] = train_df['embedding1'].apply(lambda x: \n",
    "                               np.fromstring(\n",
    "                                   x.replace('\\n','')\n",
    "                                    .replace('[','')\n",
    "                                    .replace(']','')\n",
    "                                    .replace('  ',' '), sep=' '))\n",
    "\n",
    "    train_df['similarity'] = train_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n",
    "    if 'company' in params['COLUMNS']:\n",
    "        count = len(train_df[train_df['company1'] == train_df['company2']])\n",
    "        print(\"Number of rows with the same value for company1 and company2:\", count)\n",
    "        \n",
    "\n",
    "\n",
    "# merged_df['similarity'] = merged_df.progress_apply(lambda row: cosine_similarity(row['embedding1'], row['embedding2']), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac00c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# Prepare big dataset with classification and keyword similarities \n",
    "# '''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f00ebcd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_df(df):\n",
    "    for index, row in df.iterrows():\n",
    "        print(row['snippet1'])\n",
    "        print(row['snippet2'])\n",
    "        print(\"\\n---\\n\")  # print a line for separation\n",
    "        \n",
    "def filter_cols(df):\n",
    "    desired_columns = ['snippet1', 'snippet2']\n",
    "    for col in df.columns:\n",
    "        if 'similarity' in col:\n",
    "            desired_columns.append(col)\n",
    "            if '_' in col:\n",
    "                desired_columns.append(col.split('_')[0]+'1')\n",
    "                desired_columns.append(col.split('_')[0]+'2')\n",
    "    if 'label' in df.columns:\n",
    "        desired_columns.append('label')\n",
    "    desired_columns = [col for col in desired_columns if col in df.columns]\n",
    "    df = df.loc[:, desired_columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7c12afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '../similarity-training-data/sbert-company-filtered/'\n",
    "filter_cols(test_df).to_csv(f'{prefix}test.tsv', sep='\\t')\n",
    "filter_cols(val_df).to_csv(f'{prefix}val.tsv', sep='\\t')\n",
    "filter_cols(train_df).to_csv(f'{prefix}train.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54971836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (-0.001, 0.1]     453\n",
    "# (0.1, 0.2]       1140\n",
    "# (0.2, 0.3]       1119\n",
    "# (0.3, 0.4]        827\n",
    "# (0.4, 0.5]        821\n",
    "# (0.5, 0.6]        855\n",
    "# (0.6, 0.7]        705\n",
    "# (0.7, 0.8]        423\n",
    "# (0.8, 0.9]        182\n",
    "# (0.9, 1.0]        181 \n",
    "# 1 - basically the same, the only differences are a few extra words in front or after (length check)\n",
    "\n",
    "\n",
    "# 5 The two sentences are completely equivalent, as they mean the same thing. (i.e. talk about the same company, people, event, values)\n",
    "# 4 The two sentences are mostly equivalent, but some unimportant details differ. (i.e. same company, people, event but different wording or different values)\n",
    "# 3 The two sentences are roughly equivalent, but some important information differs/missing. (e.g. same event but different company or same company and similar event)\n",
    "# 2 The two sentences are not equivalent, but share some details. (e.g. same company but different event)\n",
    "# 1 The two sentences are not equivalent, but are on the same topic.\n",
    "# 0 The two sentences are completely dissimilar.\n",
    "\n",
    "# TODO - slightly modify giving 1 when one is a substring of another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "796b4fed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>replace1</th>\n",
       "      <th>snippet1</th>\n",
       "      <th>classification1</th>\n",
       "      <th>keywords1</th>\n",
       "      <th>embedding1</th>\n",
       "      <th>id1</th>\n",
       "      <th>replace2</th>\n",
       "      <th>snippet2</th>\n",
       "      <th>classification2</th>\n",
       "      <th>keywords2</th>\n",
       "      <th>embedding2</th>\n",
       "      <th>id2</th>\n",
       "      <th>replace_no_tags1</th>\n",
       "      <th>replace_no_tags2</th>\n",
       "      <th>similarity</th>\n",
       "      <th>keyword_similarity</th>\n",
       "      <th>classification_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#PERSON, #JOBTITLE of #COMPANY, the holding co...</td>\n",
       "      <td>Sundar Pichai, CEO of Alphabet, the holding co...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[artificial intelligence]</td>\n",
       "      <td>[-0.06915243, 0.01455253, 0.024989886, -0.0257...</td>\n",
       "      <td>216067</td>\n",
       "      <td>The WisdomTree #COMPANY Value Fund ETF uses a ...</td>\n",
       "      <td>The WisdomTree International Al Enhanced Value...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[artificial intelligence]</td>\n",
       "      <td>[0.022610608, -0.12946172, -0.10784928, -0.035...</td>\n",
       "      <td>182225.0</td>\n",
       "      <td>of the holding company of recently stated in a...</td>\n",
       "      <td>The WisdomTree Value Fund ETF uses a proprieta...</td>\n",
       "      <td>0.564688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.564688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Media and Internet holding company #COMPANY is...</td>\n",
       "      <td>IAC + 1: Media and Internet holding company IA...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[artificial intelligence]</td>\n",
       "      <td>[-0.002212706, -0.042485017, -0.08343706, -0.0...</td>\n",
       "      <td>7495</td>\n",
       "      <td>#LOC’s AA-rated IT services group #COMPANY,</td>\n",
       "      <td>And Japan’s AA-rated IT services group NEC Cor...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[artificial intelligence]</td>\n",
       "      <td>[-0.016172213, -0.011744792, -0.04392088, 0.00...</td>\n",
       "      <td>149398.0</td>\n",
       "      <td>Media and Internet holding company is implemen...</td>\n",
       "      <td>AA-rated IT services group</td>\n",
       "      <td>0.167768</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.167768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>over the next few years, since the new technol...</td>\n",
       "      <td>Gen-1 is driving the next supercycle of cloud ...</td>\n",
       "      <td>[employ]</td>\n",
       "      <td>[artificial intelligence]</td>\n",
       "      <td>[-0.067335255, -0.00187942, 0.0024513635, -0.0...</td>\n",
       "      <td>49247</td>\n",
       "      <td>#COMPANY , a fast-growing #LOC-based startup w...</td>\n",
       "      <td>Hugging Face , a fast-growing New York-based s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[artificial intelligence]</td>\n",
       "      <td>[-0.0582173, -0.05877296, 0.0024392686, 0.0074...</td>\n",
       "      <td>90130.0</td>\n",
       "      <td>over the next few years, since the new technol...</td>\n",
       "      <td>, a fast-growing startup which enjoyed a valua...</td>\n",
       "      <td>0.252154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.252154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#COMPANY launches Skills-based Talent planning...</td>\n",
       "      <td>Eightfold 1 Launches Skills-Based Talent Plann...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[artificial intelligence]</td>\n",
       "      <td>[-0.056243468, -0.0053915293, 0.011274071, 0.0...</td>\n",
       "      <td>231148</td>\n",
       "      <td>As ChatGPT – developed by #COMPANY is not avai...</td>\n",
       "      <td>As 1 – developed by Microsoft-backed 1 – is no...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[chatgpt, artificial intelligence, openai]</td>\n",
       "      <td>[-0.047819942, -0.0714631, 0.047958616, -0.007...</td>\n",
       "      <td>214641.0</td>\n",
       "      <td>launches Skills-based Talent planning, empower...</td>\n",
       "      <td>As ChatGPT – developed by is not available for...</td>\n",
       "      <td>0.002902</td>\n",
       "      <td>0.729112</td>\n",
       "      <td>0.002902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The #COMPANY is designed to simplify video pro...</td>\n",
       "      <td>The 1 Video Generator is designed to simplify ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[artificial intelligence]</td>\n",
       "      <td>[-0.049500536, 0.021719, 0.010959194, -0.05274...</td>\n",
       "      <td>61653</td>\n",
       "      <td>chatbots like ChatGPT and #COMPANY Bard, may b...</td>\n",
       "      <td>the used in chatbots like 1 and 1, may be maki...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[chatgpt, google bard]</td>\n",
       "      <td>[-0.021107346, -0.07309405, 0.0432339, -0.0501...</td>\n",
       "      <td>46199.0</td>\n",
       "      <td>The is designed to simplify video production a...</td>\n",
       "      <td>chatbots like ChatGPT and Bard, may be making ...</td>\n",
       "      <td>0.122614</td>\n",
       "      <td>0.198170</td>\n",
       "      <td>0.122614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219810</th>\n",
       "      <td>Because #PERSON ’ s contract with #COMPANY inc...</td>\n",
       "      <td>Because Graham Ford ’ s contract with FCS inc...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ford]</td>\n",
       "      <td>[-0.08120356, 0.042070776, 0.094466455, 0.0682...</td>\n",
       "      <td>87539</td>\n",
       "      <td>#COMPANY and #COMPANY had agreed for a #ORG</td>\n",
       "      <td>2023 Toyota Motor Corporation and Suzuki Motor...</td>\n",
       "      <td>[agreement]</td>\n",
       "      <td>[suzuki]</td>\n",
       "      <td>[-0.0480265, 0.023894105, 0.027636005, 0.00332...</td>\n",
       "      <td>168350.0</td>\n",
       "      <td>Because ’ s contract with included both conven...</td>\n",
       "      <td>and had agreed for a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.709216</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219811</th>\n",
       "      <td>#COMPANY Get Free Report on #DATE</td>\n",
       "      <td>General Motors (GM) - Get Free Report on April...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[general motors]</td>\n",
       "      <td>[-0.059067983, 0.03613742, -0.06019407, 0.0033...</td>\n",
       "      <td>137416</td>\n",
       "      <td>The #COMPANY’s commitment to achieving full cl...</td>\n",
       "      <td>The BMW Group’s commitment to achieving full c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[bmw]</td>\n",
       "      <td>[0.017645555, 0.110141344, 0.0147629045, 0.006...</td>\n",
       "      <td>54909.0</td>\n",
       "      <td>Get Free Report on</td>\n",
       "      <td>The commitment to achieving full climate neutr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.983244</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219812</th>\n",
       "      <td>A #COMPANY expert will now assume a larger #JO...</td>\n",
       "      <td>A Volvo Bus expert will now assume a larger di...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[volvo]</td>\n",
       "      <td>[-0.03856746, 0.023804769, 0.021807693, -0.019...</td>\n",
       "      <td>110326</td>\n",
       "      <td>Reports suggest #PERSON’s private plan landed ...</td>\n",
       "      <td>Reports suggest Musk’s private plan landed in ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[tesla]</td>\n",
       "      <td>[0.060868353, 0.018765314, -0.038085416, 0.013...</td>\n",
       "      <td>119948.0</td>\n",
       "      <td>A expert will now assume a larger role with later</td>\n",
       "      <td>Reports suggest private plan landed in the Chi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.953276</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219813</th>\n",
       "      <td>Automotive manufacturer #COMPANY announced tod...</td>\n",
       "      <td>Automotive manufacturer Mercedes-Benz announce...</td>\n",
       "      <td>[market_loc]</td>\n",
       "      <td>[mercedes]</td>\n",
       "      <td>[-0.08559719, 0.06645149, 0.031096734, 0.02838...</td>\n",
       "      <td>103746</td>\n",
       "      <td>By #DATE, #COMPANY aims to have reduced the en...</td>\n",
       "      <td>By 2030, BMW aims to have reduced the entire c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[bmw]</td>\n",
       "      <td>[0.01392833, 0.1668224, 0.042746324, 0.0332090...</td>\n",
       "      <td>132008.0</td>\n",
       "      <td>Automotive manufacturer announced today new ag...</td>\n",
       "      <td>By aims to have reduced the entire carbon foot...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.954971</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219814</th>\n",
       "      <td>#LOC has won #MONEYAMOUNT in an opioid related...</td>\n",
       "      <td>Nevada has won $151 million in an opioid relat...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ford]</td>\n",
       "      <td>[-0.06050602, 0.008811979, 0.0036115053, -0.00...</td>\n",
       "      <td>146947</td>\n",
       "      <td>We have #AMOUNT #COMPANY Truck dealerships in ...</td>\n",
       "      <td>We have 19 Volvo Truck dealerships in the coun...</td>\n",
       "      <td>[part_of]</td>\n",
       "      <td>[volvo]</td>\n",
       "      <td>[0.07586177, 0.009096906, -0.013272591, -0.020...</td>\n",
       "      <td>122027.0</td>\n",
       "      <td>has won in an opioid related settlement with a...</td>\n",
       "      <td>We have Truck dealerships in the country, and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.735244</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>219815 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 replace1   \n",
       "0       #PERSON, #JOBTITLE of #COMPANY, the holding co...  \\\n",
       "1       Media and Internet holding company #COMPANY is...   \n",
       "2       over the next few years, since the new technol...   \n",
       "3       #COMPANY launches Skills-based Talent planning...   \n",
       "4       The #COMPANY is designed to simplify video pro...   \n",
       "...                                                   ...   \n",
       "219810  Because #PERSON ’ s contract with #COMPANY inc...   \n",
       "219811                  #COMPANY Get Free Report on #DATE   \n",
       "219812  A #COMPANY expert will now assume a larger #JO...   \n",
       "219813  Automotive manufacturer #COMPANY announced tod...   \n",
       "219814  #LOC has won #MONEYAMOUNT in an opioid related...   \n",
       "\n",
       "                                                 snippet1 classification1   \n",
       "0       Sundar Pichai, CEO of Alphabet, the holding co...              []  \\\n",
       "1       IAC + 1: Media and Internet holding company IA...              []   \n",
       "2       Gen-1 is driving the next supercycle of cloud ...        [employ]   \n",
       "3       Eightfold 1 Launches Skills-Based Talent Plann...              []   \n",
       "4       The 1 Video Generator is designed to simplify ...              []   \n",
       "...                                                   ...             ...   \n",
       "219810   Because Graham Ford ’ s contract with FCS inc...              []   \n",
       "219811  General Motors (GM) - Get Free Report on April...              []   \n",
       "219812  A Volvo Bus expert will now assume a larger di...              []   \n",
       "219813  Automotive manufacturer Mercedes-Benz announce...    [market_loc]   \n",
       "219814  Nevada has won $151 million in an opioid relat...              []   \n",
       "\n",
       "                        keywords1   \n",
       "0       [artificial intelligence]  \\\n",
       "1       [artificial intelligence]   \n",
       "2       [artificial intelligence]   \n",
       "3       [artificial intelligence]   \n",
       "4       [artificial intelligence]   \n",
       "...                           ...   \n",
       "219810                     [ford]   \n",
       "219811           [general motors]   \n",
       "219812                    [volvo]   \n",
       "219813                 [mercedes]   \n",
       "219814                     [ford]   \n",
       "\n",
       "                                               embedding1     id1   \n",
       "0       [-0.06915243, 0.01455253, 0.024989886, -0.0257...  216067  \\\n",
       "1       [-0.002212706, -0.042485017, -0.08343706, -0.0...    7495   \n",
       "2       [-0.067335255, -0.00187942, 0.0024513635, -0.0...   49247   \n",
       "3       [-0.056243468, -0.0053915293, 0.011274071, 0.0...  231148   \n",
       "4       [-0.049500536, 0.021719, 0.010959194, -0.05274...   61653   \n",
       "...                                                   ...     ...   \n",
       "219810  [-0.08120356, 0.042070776, 0.094466455, 0.0682...   87539   \n",
       "219811  [-0.059067983, 0.03613742, -0.06019407, 0.0033...  137416   \n",
       "219812  [-0.03856746, 0.023804769, 0.021807693, -0.019...  110326   \n",
       "219813  [-0.08559719, 0.06645149, 0.031096734, 0.02838...  103746   \n",
       "219814  [-0.06050602, 0.008811979, 0.0036115053, -0.00...  146947   \n",
       "\n",
       "                                                 replace2   \n",
       "0       The WisdomTree #COMPANY Value Fund ETF uses a ...  \\\n",
       "1             #LOC’s AA-rated IT services group #COMPANY,   \n",
       "2       #COMPANY , a fast-growing #LOC-based startup w...   \n",
       "3       As ChatGPT – developed by #COMPANY is not avai...   \n",
       "4       chatbots like ChatGPT and #COMPANY Bard, may b...   \n",
       "...                                                   ...   \n",
       "219810        #COMPANY and #COMPANY had agreed for a #ORG   \n",
       "219811  The #COMPANY’s commitment to achieving full cl...   \n",
       "219812  Reports suggest #PERSON’s private plan landed ...   \n",
       "219813  By #DATE, #COMPANY aims to have reduced the en...   \n",
       "219814  We have #AMOUNT #COMPANY Truck dealerships in ...   \n",
       "\n",
       "                                                 snippet2 classification2   \n",
       "0       The WisdomTree International Al Enhanced Value...              []  \\\n",
       "1       And Japan’s AA-rated IT services group NEC Cor...              []   \n",
       "2       Hugging Face , a fast-growing New York-based s...              []   \n",
       "3       As 1 – developed by Microsoft-backed 1 – is no...              []   \n",
       "4       the used in chatbots like 1 and 1, may be maki...              []   \n",
       "...                                                   ...             ...   \n",
       "219810  2023 Toyota Motor Corporation and Suzuki Motor...     [agreement]   \n",
       "219811  The BMW Group’s commitment to achieving full c...              []   \n",
       "219812  Reports suggest Musk’s private plan landed in ...              []   \n",
       "219813  By 2030, BMW aims to have reduced the entire c...              []   \n",
       "219814  We have 19 Volvo Truck dealerships in the coun...       [part_of]   \n",
       "\n",
       "                                         keywords2   \n",
       "0                        [artificial intelligence]  \\\n",
       "1                        [artificial intelligence]   \n",
       "2                        [artificial intelligence]   \n",
       "3       [chatgpt, artificial intelligence, openai]   \n",
       "4                           [chatgpt, google bard]   \n",
       "...                                            ...   \n",
       "219810                                    [suzuki]   \n",
       "219811                                       [bmw]   \n",
       "219812                                     [tesla]   \n",
       "219813                                       [bmw]   \n",
       "219814                                     [volvo]   \n",
       "\n",
       "                                               embedding2       id2   \n",
       "0       [0.022610608, -0.12946172, -0.10784928, -0.035...  182225.0  \\\n",
       "1       [-0.016172213, -0.011744792, -0.04392088, 0.00...  149398.0   \n",
       "2       [-0.0582173, -0.05877296, 0.0024392686, 0.0074...   90130.0   \n",
       "3       [-0.047819942, -0.0714631, 0.047958616, -0.007...  214641.0   \n",
       "4       [-0.021107346, -0.07309405, 0.0432339, -0.0501...   46199.0   \n",
       "...                                                   ...       ...   \n",
       "219810  [-0.0480265, 0.023894105, 0.027636005, 0.00332...  168350.0   \n",
       "219811  [0.017645555, 0.110141344, 0.0147629045, 0.006...   54909.0   \n",
       "219812  [0.060868353, 0.018765314, -0.038085416, 0.013...  119948.0   \n",
       "219813  [0.01392833, 0.1668224, 0.042746324, 0.0332090...  132008.0   \n",
       "219814  [0.07586177, 0.009096906, -0.013272591, -0.020...  122027.0   \n",
       "\n",
       "                                         replace_no_tags1   \n",
       "0       of the holding company of recently stated in a...  \\\n",
       "1       Media and Internet holding company is implemen...   \n",
       "2       over the next few years, since the new technol...   \n",
       "3       launches Skills-based Talent planning, empower...   \n",
       "4       The is designed to simplify video production a...   \n",
       "...                                                   ...   \n",
       "219810  Because ’ s contract with included both conven...   \n",
       "219811                                 Get Free Report on   \n",
       "219812  A expert will now assume a larger role with later   \n",
       "219813  Automotive manufacturer announced today new ag...   \n",
       "219814  has won in an opioid related settlement with a...   \n",
       "\n",
       "                                         replace_no_tags2  similarity   \n",
       "0       The WisdomTree Value Fund ETF uses a proprieta...    0.564688  \\\n",
       "1                              AA-rated IT services group    0.167768   \n",
       "2       , a fast-growing startup which enjoyed a valua...    0.252154   \n",
       "3       As ChatGPT – developed by is not available for...    0.002902   \n",
       "4       chatbots like ChatGPT and Bard, may be making ...    0.122614   \n",
       "...                                                   ...         ...   \n",
       "219810                               and had agreed for a         NaN   \n",
       "219811  The commitment to achieving full climate neutr...         NaN   \n",
       "219812  Reports suggest private plan landed in the Chi...         NaN   \n",
       "219813  By aims to have reduced the entire carbon foot...         NaN   \n",
       "219814  We have Truck dealerships in the country, and ...         NaN   \n",
       "\n",
       "        keyword_similarity  classification_similarity  \n",
       "0                 1.000000                   0.564688  \n",
       "1                 1.000000                   0.167768  \n",
       "2                 1.000000                   0.252154  \n",
       "3                 0.729112                   0.002902  \n",
       "4                 0.198170                   0.122614  \n",
       "...                    ...                        ...  \n",
       "219810            0.709216                        NaN  \n",
       "219811            0.983244                        NaN  \n",
       "219812            0.953276                        NaN  \n",
       "219813            0.954971                        NaN  \n",
       "219814            0.735244                        NaN  \n",
       "\n",
       "[219815 rows x 17 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f201c49a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
