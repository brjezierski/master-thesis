{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99f7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import classification_training_utils\n",
    "importlib.reload(classification_training_utils)\n",
    "import utils\n",
    "importlib.reload(utils)\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util\n",
    "from sentence_transformers.datasets import SentenceLabelDataset\n",
    "from sentence_transformers.readers import InputExample\n",
    "from sentence_transformers.evaluation import TripletEvaluator\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from utils import load_model, replace_nan_with, load_big_consulting_export, callback, create_replace_no_tags_embeddings\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import S3BERT.src.model_freeze as freeze\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "from classification_training_utils import get_big_consulting_df, collect_classification_labels, get_relevant_classifications, train, filter_relevant_classifications, get_top_values, get_news_df\n",
    "from transformers import EarlyStoppingCallback\n",
    "import math\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e942478b",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"EPOCHS\"] = 4\n",
    "params[\"USE_REPLACE_DATA\"] = True\n",
    "params[\"USE_ORIGINAL_DATA\"] = False\n",
    "params[\"UNFREEZE_LAYERS\"] = 2\n",
    "params[\"EXCLUDE_ENTITY_OTHER\"] = True\n",
    "params[\"INITIALIZED_MODEL\"] = None #\"brjezierski/S3BERT\" # \"intfloat/e5-small-v2\"\n",
    "params[\"OCCURENCE_CUTOFF\"] = 2\n",
    "params[\"CREATE_NEW_SPLIT\"] = False\n",
    "params[\"BATCH_SIZE\"] = 32\n",
    "\n",
    "dataset_dir = \"../classification-training-data/\" #5-9-23/\"\n",
    "sbert_model = load_model() if not params[\"INITIALIZED_MODEL\"] else load_model(model=params[\"INITIALIZED_MODEL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37b4a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MILESTONE', '^INVEST', 'PRODUCT', '^SELL', 'CLOSE-FACILITY', 'OTHER', 'PRODUCT-STOP', 'INTERNAL', '^SUPPLY', 'RATED', 'OP_LOC', 'EMPLOY-STOP', 'INVEST', 'PROBLEM-SOLUTION', 'SUE-WIN', 'SUE-END', 'EVENT', 'TECH-PATENT', 'HIRE', 'MARKET-COVERAGE', 'COWORK', '^SERVICE', '^USE', 'SERVICE', 'MARKET', 'SUPPLY', 'AGREEMENT', 'PRODUCT-ADV', 'TECH', 'ABUSE-BR', 'PRODUCT-LAUNCH', 'PROBLEM', 'AWARD', 'GROW', 'PART_OF', 'LAUNCH', 'GROW-MARKET', 'OPERATE-LAUNCH', 'USE-PROD', 'GROW-NEUTRAL', 'PR', 'PENALTY-IP', '^EMPLOY', 'LAUNCH-FACILITY', 'DECLINE-MARKET', 'STAFFING', 'ENTITY', 'ACQUIRE-ASSET', 'ACQUIRE', 'FIGHT-ABUSE-HUM', 'CORP_REPORT', 'ABUSE-LAW', 'MARKET_LOC', 'RIGHTS', 'SELL', 'USE', 'FOUNDER', 'PARTNER', 'EMPLOY-START', 'MEDIASCORE', 'STATE', 'ACTIVITY_LOC', 'ACQUIRED', 'CRITIC', 'PARTNER_JV', 'EMPLOY', 'PENALTY', 'GROW-HIRE', 'GENERAL', 'REPORT', 'COMPETENCE', '^DEAL', '^SUE', 'AWARD-SPONS', 'CAREER', 'COMPETE', '^SELL-TO', 'STAFFING-LAYOFFS', 'LEADERSHIP', 'DECLINE', 'CHARITY', 'GROW-FACILITY', 'MONETARY', 'FUNDING', 'EFFICIENCY', 'DECISION', 'DEAL', 'REGULATORY', 'GROW-PROD', 'STRATEGY', 'HQ_LOC', 'GROW-FORECAST']\n",
      "Share of rows with a non-empty list in 'classification': 2755\n",
      "Share of rows with a list of len 2 in 'classification': 21\n",
      "# of rows with a non-empty list in top_classification: 2755\n",
      "growneutral 537\n",
      "state 293\n",
      "employ 234\n",
      "product 228\n",
      "partner 192\n",
      "grow 78\n",
      "decline 68\n",
      "acquire 64\n",
      "productlaunch 64\n",
      "career 60\n",
      "employstart 59\n",
      "deal 54\n",
      "regulatory 53\n",
      "report 45\n",
      "problem 44\n",
      "growprod 37\n",
      "invest 34\n",
      "supply 34\n",
      "employstop 34\n",
      "staffinglayoffs 32\n",
      "agreement 31\n",
      "use 30\n",
      "techpatent 26\n",
      "general 25\n",
      "growmarket 24\n",
      "pr 21\n",
      "^sue 17\n",
      "op_loc 14\n",
      "service 14\n",
      "rated 14\n",
      "internal 14\n",
      "tech 13\n",
      "funding 13\n",
      "market_loc 13\n",
      "launchfacility 13\n",
      "monetary 11\n",
      "staffing 11\n",
      "competence 11\n",
      "^service 10\n",
      "corp_report 10\n",
      "efficiency 9\n",
      "leadership 9\n",
      "penalty 9\n",
      "^invest 9\n",
      "part_of 8\n",
      "cowork 8\n",
      "launch 7\n",
      "marketcoverage 7\n",
      "^supply 7\n",
      "rights 6\n",
      "hq_loc 6\n",
      "award 6\n",
      "founder 6\n",
      "event 6\n",
      "awardspons 6\n",
      "acquireasset 5\n",
      "market 5\n",
      "compete 4\n",
      "growhire 4\n",
      "mediascore 4\n",
      "growfacility 4\n",
      "charity 3\n",
      "growforecast 3\n",
      "milestone 3\n",
      "abuselaw 3\n",
      "^use 2\n",
      "productstop 2\n",
      "strategy 2\n",
      "declinemarket 2\n",
      "operatelaunch 2\n",
      "sell 2\n",
      "partner_jv 2\n",
      "closefacility 2\n",
      "penaltyip 2\n",
      "useprod 1\n",
      "^sellto 1\n",
      "^sell 1\n",
      "suewin 1\n",
      "decision 1\n",
      "^employ 1\n",
      "^deal 1\n",
      "abusebr 1\n",
      "sueend 1\n",
      "acquired 1\n",
      "critic 1\n"
     ]
    }
   ],
   "source": [
    "prefix = \"../glanos-data/embeddings/\"\n",
    "\n",
    "# with open(f'{prefix}big_consulting_2_snippet.pickle', 'rb') as f:\n",
    "with open(f'{prefix}big_consulting_2_replace_no_tags.pickle', 'rb') as f:\n",
    "    replace_no_tags_embeddings = pickle.load(f)\n",
    "\n",
    "replace_data_path = '../glanos-data/datasets/big_consulting_export_replace.tsv'\n",
    "\n",
    "if os.path.exists(replace_data_path):\n",
    "    df = pd.read_csv(replace_data_path, sep='\\t')\n",
    "#     df['embedding'] = df['snippet'].map(replace_no_tags_embeddings)\n",
    "    df = create_replace_no_tags_embeddings(df, replace_no_tags_embeddings)\n",
    "    df['embedding'] = df['replace_no_tags'].map(replace_no_tags_embeddings)\n",
    "\n",
    "df = collect_classification_labels(df, verbose=True)\n",
    "df = get_top_values(df, params)\n",
    "all_classifications, relevant_classifications = get_relevant_classifications(df, params)\n",
    "df, classifications = filter_relevant_classifications(df, all_classifications, relevant_classifications)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c3fd023",
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"WARMUP_STEPS\"] = int(len(df) * params[\"EPOCHS\"] * 0.1)  # 10% of train data\n",
    "# df['snippet'] = df['replace_no_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc72dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with one objective\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'VAL_DEV_SIZE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_fit, test_evaluator, sbert_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifications\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msbert_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/master-thesis/notebooks/classification_training_utils.py:442\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(classified_df, classifications, params, dataset_dir, sbert_model, save_model)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining with one objective\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    441\u001b[0m     steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(classified_df) \u001b[38;5;241m/\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBATCH_SIZE\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 442\u001b[0m     train_objectives, dev_evaluator, test_evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_for_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclassified_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclassifications\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msbert_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m output_path \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification_train_output/sbert-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    445\u001b[0m                datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPerformance before fine-tuning:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/master-thesis/notebooks/classification_training_utils.py:379\u001b[0m, in \u001b[0;36mprepare_for_training\u001b[0;34m(classified_df, classifications, params, dataset_dir, sbert_model)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unfreeze_layers:\n\u001b[1;32m    376\u001b[0m     freeze\u001b[38;5;241m.\u001b[39mfreeze_except_last_layers(sbert_model, unfreeze_layers)\n\u001b[1;32m    378\u001b[0m train_set, dev_set, test_set \u001b[38;5;241m=\u001b[39m get_dataset(\n\u001b[0;32m--> 379\u001b[0m     classified_df, params, classifications, create_new_split, dataset_dir, val_dev_size\u001b[38;5;241m=\u001b[39m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mVAL_DEV_SIZE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Using \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal and replacement\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_original_data \u001b[38;5;129;01mand\u001b[39;00m use_replace_data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_original_data \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m data\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    382\u001b[0m       (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, creating a new train-dev-test split\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m create_new_split \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    383\u001b[0m       (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, unfreezing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munfreeze_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m last layers\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m unfreeze_layers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m       )\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining data size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(train_set))\n",
      "\u001b[0;31mKeyError\u001b[0m: 'VAL_DEV_SIZE'"
     ]
    }
   ],
   "source": [
    "model_fit, test_evaluator, sbert_model = train(df, classifications, params, dataset_dir, sbert_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d821744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert_model.evaluate(test_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e9124",
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_model = load_model()\n",
    "frozen_model.evaluate(test_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be9d618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8907523",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
